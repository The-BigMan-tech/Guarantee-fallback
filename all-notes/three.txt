*the fov is the vertical angle in degrees that states how much of the scene the camera views

*the near and far point of a camera define the range of the camera's visibility to control whats rendered for improving performance

*inspecting the code of the built in components

*You should keep the camera definition (the actual THREE.PerspectiveCamera instance) as an internal object, and then create a class or wrapper that references this original camera object. This wrapper class will provide methods to directly modify as many modifiable properties of the camera as possible.

*The custom logic controlling the camera typically uses a limited set of core camera mutations—such as updating position, rotation, FOV, or near/far planes—but complex behaviors emerge by composing these mutations under broader logic tied to different parts of your app state or input.To create complex behaviors like spinning, following a target, zooming, or orbiting, you write higher-level logic that manipulates these properties over time or in response to user input.

*You call renderer.render(scene, camera) on every animation frame inside the animation loop because rendering produces a single still image of the current scene from the camera’s viewpoint.

*so 3d is explicit here.so under setanimation loop i can apply any transformations here but they wont be reflected in the canvas,so calling the render function each time means im telling three js to redraw the canvas based on the camera's view of the current state of the scene.i believe i must call this once after all transformations for perf

*so like disk io,rendering is the heavy operation of 3D and its best practice to batch transformations before rendering like it is for disk io and 3D objects store their properties in transformation matrixes but changes to them dont reflect to other parts of the app till i rerender it.

*a three js mesh is made up of a material applied to a geometry

*overlaying React components on your Three.js canvas as in your example gives you a powerful way to build rich, interactive in-game UIs—like inventories, menus, HUDs—using React’s full ecosystem of tools, libraries, and animation capabilities. This approach offers advantages over traditional game engines like Unity or Godot, whose built-in UI editors are often more limited or less flexible for complex, dynamic interfaces.

*You position standard HTML elements (like <div>, <span>) absolutely over the canvas using CSS and z-index.

*These points define the path of the line; lines are drawn between each consecutive pair of points but do not close the shape by connecting the last point to the first.

*A THREE.Vector3 represents a point or direction in 3D space with three coordinates: x, y, and z.When defining geometry in Three.js, you specify the positions of vertices (the corners or points of shapes) as arrays of Vector3 objects. Each Vector3 precisely locates a point in 3D space.

*so the points is an array of 3d vectors that stores the location of a point in 3d space or x,y,z cords and these points are turned to a buffered array or typed array optimized for holding large geometric data at good perf.so all three js geometry use this to convert the points into a geometry

model is a material applied to a geometry and optionally animations which is a function that defines transformations over time

*in Three.js, the scene acts as the 3D environment or world container for all objects you want to render. Any model or object that you want to appear in the rendered output must be added to the scene (directly or indirectly).

in the browser,js can only access files exposed to it from the web server which is in the public folder of the project unless in a tauri env where it can use a secured rust backend instead

*Create a new RoomEnvironment scene, which is a simple, neutral room-like environment with walls and ceiling that emits light and reflections.

*Pmrem preprocesses environment maps to efficiently simulate realistic reflections and lighting for different roughness levels.it precomputes filtered versions of the environment map at multiple roughness levels.This allows materials to quickly sample the correct reflection blur based on their roughness without expensive runtime filtering which improves performance and visual quality of reflections and ambient lighting in PBR.

*It is a texture (often a cube map) that encodes the surrounding environment’s light and color information from all directions.On models: The environment map is sampled by the material’s shader to produce reflections and ambient lighting effects on the model’s surface.This means shiny or metallic materials reflect the environment realistically, and even rough surfaces receive ambient light from the environment, making them look more natural.

*so is it that normal light is just required to see the model.they create highlights, shadows, and shading that define the shape and depth of objects but env map is required to produce accurate reflections on those models by simulating light at different directions for a particular model depending on its roughness

*The camera’s fov property defines the vertical field of view (in degrees).The aspect ratio is usually set to the canvas width divided by its height (window.innerWidth / window.innerHeight).This aspect ratio determines how wide the camera’s horizontal field of view is, based on the fixed vertical FOV.

*both browser 3D rendering and Godot’s 3D rendering ultimately display images on 2D planes (the screen), creating the illusion of 3D by projecting 3D models onto 2D surfaces. However, Godot provides a full 3D scene graph and retains all the underlying 3D data—such as meshes, planes, nodes, and materials—allowing you to inspect and manipulate each model's components directly within its editor or runtime environment.
*In contrast, browsers render 3D scenes via WebGL or WebGPU onto a 2D canvas element, which only holds the final rasterized pixels and does not expose the original 3D models or their geometry for inspection. The canvas is essentially a flat drawing surface without the scene graph or mesh data that Godot maintains internally. This is why you can inspect individual 3D models and their planes in Godot but only see the 2D canvas output in a browser

scene graph
canvas drawing

Graphic apis

*a graphic api sends geometry data as rendering instructions to the gpu.the gpu uses this information to render the 3d model on a 2d surface like the screen or canvas.They take geometric data types like triangles and convert them into low level gpu instructions like an assembler

Shaders programming
parallel programming
3D programming

RAM
Virtual RAM
Video RAM

GPU instructions
3D api--geometric datatypes
3D binding--just an interface over a 3d api
3D library--includes a scene graph
3D engine--code driven
Game engine
Modelling software

webGL,openGL,DirectX,Vulkan
three js,babylon,blend4web,lwjgl

*3D libraries like Three.js, Babylon.js, and game engines serve as abstractions over low-level geometric data types and rendering APIs to simplify 3D production.

scene graph
rendering

A scene graph is a hierarchical, structured representation of a 3D scene that organizes objects (nodes), their spatial transformations, and relationships in a tree or graph form

a 3d api is just above making raw gpu commands while a scene graph allows one to interact with graphics in an object-oriented manner

chunk based rendering and culling
The game logic and rendering are decoupled; entities and blocks have their own update and render methods, but there is no global hierarchical scene graph managing all objects.

*a scene graph is a centralized abstraction like the dom but minecraft uses decentralized abstractions.It provides a single source of truth for the scene’s spatial and semantic organization, enabling efficient traversal, rendering, culling, and interaction management.

a scene graph allows for relationships.without it,models will just be rendered on the screen
spatial partition and culling--grids,chunks

localized entity system--ecs
pose stack,transformation stack

rendering
relationships
model inspection

final image
localized entity system
scene graph

data structure or relationship system
renderer

geometric datatypes
final image

*so its from geometric datatypes-->final image but a scene graph or relationship system like the localized entity saves the geometric data of each component to model relationships.

*A scene graph or localized entity system stores not just the geometric data but also the hierarchical and spatial relationships between components or models.It saves which parts belong to which parent objects, how they are transformed relative to each other, and how they interact, effectively organizing geometric data into meaningful structures.

*Geometric data represents the shape and appearance of 3D objects. This includes vertices, edges, faces, curves, surfaces, and points that define the object’s form
*Transformation matrices hold data used to transform those objects properties

geometric components

*A scene graph or similar system (like a localized entity system) stores and organizes the geometric data of each model component individually, along with their spatial and semantic relationships.This organization allows the code to draw particular geometric components in isolation, enabling you to select, inspect, and manipulate individual parts of a model rather than just rendering the whole scene as a flat image.The final rendered image you see on screen is the result of composing many such individual components, transformed and drawn many times per second to create a smooth 3D view.

running frames continuously simulates motion

*moving around in a 3d world requires a transformation of the camera over time.The rendering process redraws the entire scene relative to the camera’s current transform every frame (tick), producing the final 2D image on the screen.

*When you move or transform the camera, the scene graph is traversed each frame during rendering (renderer.render(scene, camera)), applying all object and camera transformations to compute their positions relative to the new camera viewpoint.This traversal and hierarchical transformation propagation is what allows the renderer to "bubble" the effects of camera movement through the scene, updating the final rendered image accordingly.Without the scene graph, you would need to manually track and apply all transformations and redraw calls, which is complex and error-prone.

*Without a Virtual DOM, using vanilla JavaScript means you must manually update the real DOM whenever something changes. This can be tedious and error-prone because you have to know exactly which elements to update, and other parts of the DOM won’t automatically react or update relative to those changes.

*so its the same for 3d.without a scene graph,if i move the camera for example,i also have to manually update all the other elements relative to it so that they too can be rerendered so the better way is to use a data structure over this layer and a runtime that lookups and manages this structure.so the app or game doesnt make direct graphic instructions but one on the this structure which makes it easy for some runtime to apply transformations across all elements relative to another when that element transforms

*a scene graph in 3D acts like a reactive hierarchical data structure that makes modeling relationships between objects natural and efficient, because transformations and updates propagate automatically through the graph.

camera is a view into the image

models,nodes

transformations

render loop

attatching event listeners to the target

*tabIndex controls focus levels
*z-index controls overlay levels

transformation variable mutation
continuously rendered function--uses a formula

the animation loop should be the only place where transformations are actually applied centralizing state and smoothing out transformations ensuring that they are applied predictably if multiple event listeners transform the model abruptly

It’s essentially an “update pattern” or “state interpolation pattern”, where you maintain a target state as a separate variable and have a dedicated update function that applies smooth transitions (interpolations) from the current state to the target state each frame.

direct transformation--uses formula on every loop
target transformation pattern

*the fundamental transformations
scaling
translation
rotation

*Weighting is not a transformation itself but rather a scalar value that controls the influence or strength of a transformation or effect.

*so the pattern most games use for transforming objects is to modify a target transformation and in a single function,they use the difference between the two states to transform the object and applying additional things like weights such as speed.Physical quantities like speed, force, acceleration, or damping act as these weight variables that modulate the transformation updates, making motion feel natural and physically plausible.

a model is an object composed of a material/tecture applied to a geometry construct

*an AmbientLight in Three.js produces light that illuminates all objects equally from every direction without casting shadows or having a specific direction. while a directional light casts rays of light at a particular direction

keybinds to transformations
Orbit controls

*so what im experiencing is a conflicting transformation which happens when two different parts of an app update the transformations of a single object themselves which will almost always happen out of sync.my keybinds dont conflict with themselves because i used the target transformation pattern but orbit controls is doing its own transformations on my camera

clumsy rotation
natural rotation

euler based rotation
quaternion-based rotation
clamp the rotation
rotate independent 3d object 
nesting the camera 
YXZ rotation
applying quaternion vector

rotating while moving

*Yaw is the horizontal rotation (looking left and right), which is a rotation around the Y axis in Three.js’s coordinate system.
*Pitch is the vertical rotation (looking up and down), which is a rotation around the X axis.

*so yaw is a horizontal rotation but rotates around its cross axis,y/vertical axis

*so the add method in three js is to nest a three js object in another

*Rotating the camera directly (modifying camera.rotation or camera.quaternion) changes the camera’s orientation in world space every frame. This can sometimes cause complex recalculations in the rendering pipeline, especially if done abruptly or with conflicting inputs, potentially leading to jitter or performance hiccups

*The camera is a child of pitchObject, which is itself a child of yawObject.When you rotate yawObject (around the Y axis), it rotates the entire subtree, including pitchObject and the camera.Because the camera inherits transformations from its parents, rotating yawObject effectively rotates the camera horizontally (yaw).

*so to rotate an object horizontally to look left or right,i actually rotate horizontally but the rotation orbits the y-axis which makes the object remain in its posture while rotating it to look up and down will orbit the rotation about the x-axis thats why yaw is aplied at the y-axis but controls horizontal rotation and vice versa for pitch

*so directly translating the camera is possible because it just needs to redraw the scene relative to how close or far they are from the camera but rotating the camera causes complex recalculations as the renderer has to redraw the scene according to the new orientation which can cause clumsy rotations if done abruptly but by nesting the camera in a normal 3d object which has more controlled rotations,i can effectively rotate the 3d object only and the camera being part of that object rotates along with it without it having to rotate by itself.so the renderer still redraws the scene but it doesnt have to handle calculating camera rotations before doing so.and by separating the 3d object responsible for rotations into separate 3d objects,i can prevent unexpected rotations/gimbal lock which happens when the pitch and yaw rotations of the same object interfere but to treat it as a single unit again,i nest one of them inside the other mmaking one rotation always relative to the other.but the pitch object is preferrable as it ensures up/down movement stays aligned with the camera’s horizontal facing.so we clamp the pitch not to prevent conflict but to prevent the camera from rotating unbounded in the y-axis which can cause an unnatural upside down rotation.This ensures the camera behaves like a real-world head or eye that can look up and down but not spin completely over itself.

*so yaw rotation that controls facing left and right which orbits the y-axis should come first because naturally,we turn to look left or right before looking up or down.so doing yaw first is better and more natural so that looking up or down is relative to us looking left or right

*Three js handles rotations as quaternions internally on all 3d objects but exposing a simple euler rotation interface.so separating them even though quaternions already prevents gimbal locks helps me to focus on rotating one object focusing on one axis at a time which allows for isolated and simplified logic for each of the axis

*when i rotate the camera using the yaw and pitch objects,i rotate it orientation in the world space but when i move it,i move it along the world axis not from the camera's local axis which comes from its own different point of view as a result of rotation

*so its better to use the vector datatype to store x,y,z cords data for points or transformations than separate individual numerical data types.

*so add on a vector performs a vector addition while for 3d objects,it nests them under its hierarchy.using a vector allows me to leverage this over arithmetic

*so lerp is a vector operation that updates the current data to the target using a weight.its the cleaner form of calculating the delta and weighting it myself using arithmetic on multiple numerical types representing the cords

*so apply quaternion is a vector operation that updates the cords data of a vector to align with that of another which in this case is used to align the rotation of the object with that of the camera so that ist translation isnt on the fixed world axis

*A quaternion represents a rotation in 3D space, encoding an axis and an angle of rotation.

*so the cords of a vector rep as (x,y,z) only hold point or direction data.the rotation data is rep separately and internally as quaternion but i can interface with the rotation as arithmetic values(Euler angles).Quaternions can be applied to vectors to rotate them, transforming their coordinates accordingly.

*so for both position,translation and rotation data,i can manually manage numeric values and use them to update the corresponding properties of the objects but using a vector data structure for the postition and translation while an euler object for rotation is better


*so i have to initialize the vectrs even afer defining them to align them with the camera’s current actual position and rotation in the scene.This ensures that the target values start synchronized with the camera’s real state, preventing sudden jumps or erratic behavior when you begin moving or rotating the camera.Without this initialization, targetPosition and targetRotation would start at default values (often zero vectors or zero Euler angles), which likely differ from the camera’s current transform.

*pointer lock behavior depends on browser and OS implementations, and certain input combinations—like simultaneous key presses and trackpad gestures—can cause pointer lock to be lost or mousemove events to be suppressed

*Pointer Lock API delivers raw device input (movementX, movementY), which reflects the actual physical mouse or trackpad movement.
*Programmatic rotation (e.g., setting Euler angles or quaternions) can simulate rotation visually but does not generate real pointer lock mousemove events.
*Browsers do not allow synthetic mousemove events to emulate pointer lock input for security and usability reasons.
*Without real pointer lock input, you lose the continuous, unrestricted relative movement that pointer lock provides (e.g., unlimited yaw rotation without cursor hitting screen edges).

^ill load plugins as ts src files directly cuz of security reasons and compile it to wasm before the app starts if possible to do securely to get the best performance.the wasm files can be temprorary and deleted immediately once the app closes.

*so in first person,the camera controls the movement and the player moves relative to the player but in thirs person,the model controls the movement and the camera orbits the player moving relative to it

*so what i was battling not just in my app but my whole desktop with the other applications including minecraft when i couldnt move my cursor when holding down a key was because of a recommended and safe feature called palm check which to prevent accidental cursor movements while using the keyboard.its a safety against the malfunctioning touchpads but it doesnt enforce this on an external mouse

*i fixed it.so the underlying problem is that since ive nested the camera in an object and applied rotation on that object to affect the camera,in order to get a rotation that applies a spinning effect which is where the camera rotates about its origin but if the camera is at an offset from tjis pivot,it orbits around it giving the visual effect that the world is rotating aound it.i displaced the camera by 5 so this caused the offset.after removing it,it went back to the spinning effect.this means that the displacement should be at the yaw object not the camera

*The order in which you apply transformations, and the pivot points of those transformations, drastically affect the final visual result.

copy position

*does the environment of scene setups break on procedural generation.is that why we have the render loop so that it can call the env functions with parameters to update the env based on conditions

*3d updates happens in two places;inside and outside the animation loop

*so the sun is a shading effect not a texture.so there are two ways to create sun,clouds etc.by either using textures or shader effects.so shader effects are difficult to customize unlike textures because:Shader code is written in GLSL, a specialized GPU programming language, which requires understanding of graphics pipeline, math, and GPU programming concepts.

*so a texture is an image that can be applied to geometric constructs or sprites.geomertic constructs define the shape of 3d models while sprites are 2d planes where a texture can be applied to define a 2d object.i believe the shape of that object is defined by the shape of the plane used.they always face the camera

*A material is a set of properties and shaders that determine how light interacts with the surface of a 3D object. Materials use textures as inputs (e.g., color maps) but also define shading models, reflectivity, transparency, and other visual effects

*In TypeScript, the Partial<T> utility type is used to create a type that represents all properties of type T as optional

*This,this behemoth of unsafety is called assertion chaining:{ qualityPreset: 'low', coverage: 0.4 } as unknown as THREE.Camera

^Create a good gui control for tuning variables for my web app

*so by default,plane geometries lie on the xy plane which when very thin,wont be visible to the camera which is because the plane stands vertically when viewed from a typical camera positioned looking along the negative Z axis.but by rotating it to the xz plane instead,it can become visible to the camera.To make the plane act as a flat ground or terrain, you need to rotate it so it lies horizontally on the XZ plane. This is done by rotating it around the X axis by pi/2 rads.The negative sign means rotation is clockwise when looking along the positive X axis (right-hand rule).

*so three js and 3d libraries in general use radians as their unit for measuring angles instead of degrees so i always have to express my desired angle from degrees to radian terms which involves pi.they use radians because they are the standard unit in mathematics and computer graphics for angular measurements.

*a player is composed of a model and a camera

*The distinction between THREE.Group and THREE.Object3D is subtle because Group is actually a subclass of Object3D, designed specifically to make grouping multiple objects clearer and more semantically explicit.

*Both Group and Object3D can nest objects and serve as containers.The difference is mostly semantic and organizational.Group is explicitly meant for grouping and makes your code clearer and easier to understand.Object3D is more generic and can represent any 3D entity (including ones with geometry or special behavior).

*You should not directly transform an object in the render loop but rather interpolate through the change between the previous position and the target

*so the reason why i directly use the quaternion in the camera cuz the camera is only doing one type of rotation which is looking up or down but the player despite only controlling horizontal rotation for now,may require more complex rotations later so its best to use euler angles and only convert to quaternions when interpolating

axis-angle
euler 
quaternion

*Axis–angle representation describes a rotation by specifying:An axis (a unit vector indicating the direction around which to rotate).An angle (how much to rotate around that axis, in radians or degrees).Its mostly a bride representation before being converted to euler or quaternion.axis–angle representation is fundamentally an intuitive way to express a rotation by specifying how much to rotate (the angle) and around which axis (a unit vector). It is not strictly required for rotation to function, but it is a very convenient and geometric way to describe rotations.

*Vectors for translation
*Rotation angles for rotation

*Euler angles are not vectors in the usual sense, but rather a set of three angles representing sequential rotations about coordinate axes (commonly X, Y, Z).They describe orientation by specifying how much to rotate around each axis, in a fixed order (e.g., rotate around Y, then X, then Z).Because they are three scalar values, sometimes people loosely think of them as a 3D vector of angles, but mathematically they represent a composition of rotations, not a direction vector.

*Multiplying quaternions applies rotations:targetQuaternion = pitchChange * targetQuaternion means apply the new incremental rotation pitchChange before the existing rotation.Quaternion multiplication composes rotations smoothly and avoids issues like gimbal lock.

why im using quaternions

*rotation matrices are a fundamental underlying data structure used by graphics APIs (including Three.js and WebGL) to represent and apply rotations in 3D space. They are usually not directly exposed as simple rotation angles to your application but are used internally to perform transformations efficiently.

high-level graphic data structures--vectors
internal data structures--matrixes
geometric data types

number
vector
Tuple
four-component number system

*so eulers are a set of x,y,x cords.They are more of tuples.they are complete and specify exact rotations so they require a linear progression on each component to the target values which can cause dragginess on smooth rotations like 360 rotations because angles wrap around at ±180°, causing sudden jumps in interpolation.Quaternions are a four-component number system: one real part (w) and three imaginary parts (x, y, z). using multiplication instead of addition meaning that it can smoothly interpolate across 360 rotations.They represent rotations as a single rotation around an axis by an angle, encoded compactly and without ambiguity.



*there is a top level animation guard clause before playing any new animation to ensure that all necessary animation components are properly initialized before attempting to change or play animations. This prevents runtime errors or unexpected behavior if, for example, the mixer or animation actions are not yet ready or failed to load.

*The AnimationMixer is the central controller for playing animations on a specific 3D object (usually a model or a group).It manages one or more AnimationActions and updates their playback over time.You must call mixer.update(deltaTime) every frame to advance the animation playback in sync with your render loop.

*An AnimationClip represents a reusable animation sequence (e.g., "idle", "walk", "run").Clips are the raw animation data but are not played directly.

*An AnimationAction is a playable instance of an AnimationClip controlled by the AnimationMixer.Actions can be played, paused, stopped, reset, and crossfaded.You call action.play() to start an animation and action.crossFadeTo(otherAction, duration) to smoothly transition between animations.

*a Pointer to the current playing action (currentAction) to manage transitions properly.It acts as a guard to prevent repeatedly replaying the same animation when it is already active. This avoids unnecessary resets or restarts of animations like walking, which might be triggered continuously while the movement key is held down.

*Pointers to all the animation actions.so its the pointers i saved to those animation actions when the module loaded that i use to play the animation i want not the current action pointer.

feedback loop leads to best practices/scarcity of pre-built solutions
pre-built solutions
no abstraction noise

*so cannon-es is just on maintenance development while rapier-3d is writte inr rust,more ecosystem support,more accurate and efficient.rapier-3d is the better choice

*so i can write or use any battery/engine in rust and transpile it to wasm to use it in js.the only thing is to create js wrappers around those wasm calls if they arent provided by the developer



character animations
physics
terrain and procedural generation
 
render-loop/game-loop

rigid body--static,dynamic,kinematic

rigid bodies state the dynamics of a solid 
a collider is used to state a solid;s collision shape

?Note that rigid-bodies are only responsible for the dynamics and kinematics of the solid
?Typically, the inertia and center of mass are automatically set to the inertia and center of mass resulting from the shapes of the colliders attached to the rigid-body

?cel-shading
?how can i load my model here in three js while rendering the material if blockbench embeds the material in the model export

?how to make the canvas drawing sharper and less jaged on the edges.fogging out the env
?orthographic camera

*describe a rigid body
*add it to the world

*a fixed rigid body is one that cannot move.its like it has an infinite mass and cannot be affected by any force
*a dynamic rigit body is one that is affected by external forces and contacts
*a position based kinematic body is one that its position shouldnt be affected by the physics engine
*a velocity based kinematic body is one that its velocity shouldnt be affected by the physics engine


*Most games involve bodies behaving in ways that defy the laws of physics: floating platforms, elevators, playable characters, etc. This is why kinematic bodies exist: they offer a total control over the body’s trajectory since they are completely immune to forces or impulses (like gravity, contacts, joints).

kinemtic bodies are physic bodies that are totally controlled by the program not the physics engine

event listeners
i can make models in three js to interact using global variables as state or maybe i must build a state mechanism with reducers on a global state object

the godot editor is a scene editor
^essentially vs just

*platformaer-2d
*parkour--3d

scene
physics world

*in 3d,outdoor scenes have one light source which is a directional light from the sun or moon while rooms have many ambient lights


*The main reason why axis by axis rotation from euler angles isnt practical is that there isn't a unique way to construct an orientation from the angles. There isn't a standard mathematical function that takes all the angles together and produces an actual 3D rotation. The only way an orientation can be produced from angles is to rotate the object angle by angle, in an arbitrary order.

*This could be done by first rotating in X, then Y and then in Z. Alternatively, you could first rotate in Y, then in Z and finally in X. Anything works, but depending on the order, the final orientation of the object will not necessarily be the same. Indeed, this means that there are several ways to construct an orientation from 3 different angles, depending on the order of the rotations

*euler is best as yxz cus you naturally look up or down before rotating left or right

*the effect on orientation from euler angle rotation is heavily dependent on axis rotation order

euler describes how a rotation can go to another by rotating the indivisual components to get there but it isnt the shortest path of rotation even though the destination angles are equivalent. i.e 270 to 360 is not visually the same as 270 to 0 but quaternions use a mathemetical function to describe the shortest path for a rotation change not per component.rotation is done by multiplying.in quaternions,you dont think in angles.the quaternion function has the three axis components/its virtual components point in the direction they were rotated to from the real component which is the origin to get a unique rotation.over time,quaternions loose precision as they are rotated because of floating point errors so they cant be exactly 90 degrees from each other.a way to fix this is to normalize it

the direction of rotation is perpendicular to the axis where its rotating about

todo:the godot doc has a lot about 3d programming.use it to learn about 3d programming but use its three js counterpart

todo:Rapier physics engine documentation

*rotation can be done relative to the world space which is fixed or the object's space

shading language--glsl
RGBA,HSV,MERH
model/geometry,material,texture,animations,effects--particle,sound

gridmap is the 3d version of a tile map

a geometry is an array of 3d position vectors(points or vertices) that define the shape of a model in 3d space

vector math is under linear algebra

a vector is a data structure that states the relative direction(cords) and magnitude(angle) from an origin.it can be represented programmatically as an object,array or tuple.They are always relative.You can use either method (x and y coordinates or angle and magnitude) to refer to a vector, but for convenience, programmers typically use the coordinate notation.because programmatically,vectors typically represent cords,they are only used for translations and scale but for rotations,euler angles and quaternion objects are used

co-ordinates
vectors
matrices

reactive programming
3d programming

material
shaders

many programming fields require maths--numerical computing,3d programming,data science programming,machine learning

many others dont require math but domain specific software knowldege like algorithms,data strutcures and paradigms--desktop,mobile,web app,websites,systems programming,network programming,backend dev,etc

multiplayer uses webRTC or web socket server but webRTC is better for real-time game scenarios

3d gizmos

pbr,flat rendering
shader

in blockbench and 3d in general,pieces will be affected if they get moved into a parent with transformations.

todo:read the godot doc on maths and read physics with rapier

the visual object and physics object are separate entities so you dont connect them but synchronize them

synchronize position of the rigid body with the model
move the rigid body

for player movement,use impuses as forces can accumulate over time and can cause unwnated acceleration

epsilon is used to treat precision issues

velocity-based detection
position-based detection--unsuitable for platforms
collision-based detection
raycast detection
collider query detection

*so the pattern i used which is through the flag ensures that i dont step upin the same movement where i collide with the object.i move,collide,update position,check if i should step and the next movement i make is a step up not that i move and step up simultaneously

*so i have to compute the forward vectpr whenever needed.not directly when updating the player psoition cuz the player's position should always be the world space but only forces should be direction based

*so the physics engine may not always work the way i expect it which isnt necessarily because of my logic but internal physics calculations.so things like this requires tuning not changing core logic.this means that the physis engine is very tedious to work with

*The physics engine's internal calculations and constraints mean that how and when you apply forces—like linear velocity and impulses—greatly affects the behavior.

*Dozens of sensitive tunable variables (e.g., friction, restitution, velocity magnitudes, step detection distances) that must be adjusted cautiously because small changes can drastically affect behavior.

*Yes, your diagnosis is correct: the jitter was caused by small precision differences between the terrain mesh’s position and its physics rigid body’s translation, even though they seem to start at the same coordinates by default. Such tiny discrepancies are common in physics-rendering integrations and can cause visible jitter or vibration.

*so the maxradius dictates how far they are distributed in spherical sistribution and they ust at least by twice the size of the geometry to prevent overlapping

*Spatial distrubution methods
random distribution
cube distribution
spherical distribution
polar distribution
poisson disk sampling


procedural generaion

they focus on point.they dont accound for volume so they can be well spaced but with overlaps

Math.random() generates a value between 0 and 1.
Multiplying by (maxHeight - minHeight) scales it to the desired height range.
Adding minHeight shifts the range to start at minHeight.

You "lift" the cube by adding half its height to the Y position because of how Three.js BoxGeometry is defined and positioned:
BoxGeometry is centered at its origin (0,0,0) by default, so its vertical center is at Y=0.
This means the cube extends equally above and below Y=0: half the height above, half below.
To place the cube so its base sits exactly on the ground plane (e.g., Y=0 or fixedY), you must raise it by half its height.
Why add half the height to Y?
If you set the cube's position Y to the ground level (say, 0 or fixedY), the cube will be centered there, so half of it will be below ground and half above.
When you position the cube vertically, you must remember that Three.js BoxGeometry is centered at the origin. This means:

The cube extends half the height above its position.y

And half the height below its position.y

To make the cube’s base sit on the ground plane (fixedY), you need to lift the cube by half its height:

The reason you add half the height to the cube’s Y position but use the full minHeight when calculating the cube’s height range comes down to the difference between:

Determining the cube’s height value, which is a scalar length (full height of the cube), and

Positioning the cube in 3D space, where the geometry is centered at its origin.

the scalar and random multiplication alone won’t produce values in the desired range unless you add the minimum value explicitly — the addition of minHeight is essential, not just a bonus.

so the min and the max without adding the min height will be off by min height units which if ignored for a large value can be a problem

so the calculation just gets the range between the min and max height.its a difference not a full rep of the height.so i have to add this range to the min height to enter the range i want and what this does is that the random func is just a weight on the range ensuring that the range is weighted differently across the loop

*Character controller
rule based character controller
kinematic based character controller
dynamic based character controller

jumping is a bit unresponsive cuz of ground calculation

typeof is for union while instanceof checks against unknown or any type

website copiers can only download static html.thats why cyotek web copy fails to download some wbesites because some of them run entirely or a lot on javascript

point casting is more optimized than shape casting

order of operation

*Yes, visual debuggers in Three.js (such as hitboxes, bounding boxes, or point coloring) often require small manual offsets to appear visually accurate because of several factors like model pivot points, floating-point precision, or coordinate system differences. These offsets help align the visual debugging aids with the actual geometry or interactive parts of the model.

*Movement in games is usually processed per frame, not instantly.Pressing a key once typically sets a velocity or triggers movement logic that updates the character’s position gradually over multiple frames.The distance covered in a single frame is small because frames are very short (e.g., ~16.7ms at 60 FPS).So even if your velocity is high (e.g., 30 or 40 units/s), the actual position change in one frame is velocity × deltaTime, which is a fraction of a unit (e.g., ~0.5 units).so frame is the accurate way to measure the distance.if i were to do it per second,it would have been higher per difference but it wont be accurate to use in computation

*Yes, using velocity × deltaTime + margin to compute detection distance helps prevent overshooting obstacles because:You’re predicting how far the character will move in the next frame plus a buffer.This means the detection zone moves dynamically with the character’s speed and frame rate.It avoids detecting obstacles too late (which causes clipping) or too early (which causes unnecessary stopping).It aligns with best practices in game physics and collision detection to maintain smooth and responsive movement.

three-step clearance check detection
its now an unblocking three-step clearance detection mechanism cuz each step is tied to the clearance of the previous one which means that computation that arent necessary dont block the character update loop

game framework