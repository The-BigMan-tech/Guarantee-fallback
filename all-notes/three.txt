*the fov is the vertical angle in degrees that states how much of the scene the camera views

*the near and far point of a camera define the range of the camera's visibility to control whats rendered for improving performance

*inspecting the code of the built in components

*You should keep the camera definition (the actual THREE.PerspectiveCamera instance) as an internal object, and then create a class or wrapper that references this original camera object. This wrapper class will provide methods to directly modify as many modifiable properties of the camera as possible.

*The custom logic controlling the camera typically uses a limited set of core camera mutations—such as updating position, rotation, FOV, or near/far planes—but complex behaviors emerge by composing these mutations under broader logic tied to different parts of your app state or input.To create complex behaviors like spinning, following a target, zooming, or orbiting, you write higher-level logic that manipulates these properties over time or in response to user input.

*You call renderer.render(scene, camera) on every animation frame inside the animation loop because rendering produces a single still image of the current scene from the camera’s viewpoint.

*so 3d is explicit here.so under setanimation loop i can apply any transformations here but they wont be reflected in the canvas,so calling the render function each time means im telling three js to redraw the canvas based on the camera's view of the current state of the scene.i believe i must call this once after all transformations for perf

*so like disk io,rendering is the heavy operation of 3D and its best practice to batch transformations before rendering like it is for disk io and 3D objects store their properties in transformation matrixes but changes to them dont reflect to other parts of the app till i rerender it.

*a three js mesh is made up of a material applied to a geometry

*overlaying React components on your Three.js canvas as in your example gives you a powerful way to build rich, interactive in-game UIs—like inventories, menus, HUDs—using React’s full ecosystem of tools, libraries, and animation capabilities. This approach offers advantages over traditional game engines like Unity or Godot, whose built-in UI editors are often more limited or less flexible for complex, dynamic interfaces.

*You position standard HTML elements (like <div>, <span>) absolutely over the canvas using CSS and z-index.

*These points define the path of the line; lines are drawn between each consecutive pair of points but do not close the shape by connecting the last point to the first.

*A THREE.Vector3 represents a point or direction in 3D space with three coordinates: x, y, and z.When defining geometry in Three.js, you specify the positions of vertices (the corners or points of shapes) as arrays of Vector3 objects. Each Vector3 precisely locates a point in 3D space.

*so the points is an array of 3d vectors that stores the location of a point in 3d space or x,y,z cords and these points are turned to a buffered array or typed array optimized for holding large geometric data at good perf.so all three js geometry use this to convert the points into a geometry

model is a material applied to a geometry and optionally animations which is a function that defines transformations over time

*in Three.js, the scene acts as the 3D environment or world container for all objects you want to render. Any model or object that you want to appear in the rendered output must be added to the scene (directly or indirectly).

in the browser,js can only access files exposed to it from the web server which is in the public folder of the project unless in a tauri env where it can use a secured rust backend instead

*Create a new RoomEnvironment scene, which is a simple, neutral room-like environment with walls and ceiling that emits light and reflections.

*Pmrem preprocesses environment maps to efficiently simulate realistic reflections and lighting for different roughness levels.it precomputes filtered versions of the environment map at multiple roughness levels.This allows materials to quickly sample the correct reflection blur based on their roughness without expensive runtime filtering which improves performance and visual quality of reflections and ambient lighting in PBR.

*It is a texture (often a cube map) that encodes the surrounding environment’s light and color information from all directions.On models: The environment map is sampled by the material’s shader to produce reflections and ambient lighting effects on the model’s surface.This means shiny or metallic materials reflect the environment realistically, and even rough surfaces receive ambient light from the environment, making them look more natural.

*so is it that normal light is just required to see the model.they create highlights, shadows, and shading that define the shape and depth of objects but env map is required to produce accurate reflections on those models by simulating light at different directions for a particular model depending on its roughness

*The camera’s fov property defines the vertical field of view (in degrees).The aspect ratio is usually set to the canvas width divided by its height (window.innerWidth / window.innerHeight).This aspect ratio determines how wide the camera’s horizontal field of view is, based on the fixed vertical FOV.

*both browser 3D rendering and Godot’s 3D rendering ultimately display images on 2D planes (the screen), creating the illusion of 3D by projecting 3D models onto 2D surfaces. However, Godot provides a full 3D scene graph and retains all the underlying 3D data—such as meshes, planes, nodes, and materials—allowing you to inspect and manipulate each model's components directly within its editor or runtime environment.
*In contrast, browsers render 3D scenes via WebGL or WebGPU onto a 2D canvas element, which only holds the final rasterized pixels and does not expose the original 3D models or their geometry for inspection. The canvas is essentially a flat drawing surface without the scene graph or mesh data that Godot maintains internally. This is why you can inspect individual 3D models and their planes in Godot but only see the 2D canvas output in a browser

scene graph
canvas drawing

Graphic apis

*a graphic api sends geometry data as rendering instructions to the gpu.the gpu uses this information to render the 3d model on a 2d surface like the screen or canvas.They take geometric data types like triangles and convert them into low level gpu instructions like an assembler

glsl
parallel computing
rendering

RAM
Virtual RAM
Video RAM

GPU instructions
3D api--geometric datatypes
3D binding--just an interface over a 3d api
3D library--includes a scene graph
3D engine--code driven
Game engine
Modelling software

webGL,openGL,DirectX,Vulkan
three js,babylon,blend4web,lwjgl

*3D libraries like Three.js, Babylon.js, and game engines serve as abstractions over low-level geometric data types and rendering APIs to simplify 3D production.

scene graph
rendering

A scene graph is a hierarchical, structured representation of a 3D scene that organizes objects (nodes), their spatial transformations, and relationships in a tree or graph form

a 3d api is just above making raw gpu commands while a scene graph allows one to interact with graphics in an object-oriented manner

chunk based rendering and culling
The game logic and rendering are decoupled; entities and blocks have their own update and render methods, but there is no global hierarchical scene graph managing all objects.

*a scene graph is a centralized abstraction like the dom but minecraft uses decentralized abstractions.It provides a single source of truth for the scene’s spatial and semantic organization, enabling efficient traversal, rendering, culling, and interaction management.

a scene graph allows for relationships.without it,models will just be rendered on the screen
spatial partition and culling--grids,chunks

localized entity system--ecs
pose stack,transformation stack

rendering
relationships
model inspection

final image
localized entity system
scene graph

data structure or relationship system
renderer

geometric datatypes
final image

*so its from geometric datatypes-->final image but a scene graph or relationship system like the localized entity saves the geometric data of each component to model relationships.

*A scene graph or localized entity system stores not just the geometric data but also the hierarchical and spatial relationships between components or models.It saves which parts belong to which parent objects, how they are transformed relative to each other, and how they interact, effectively organizing geometric data into meaningful structures.

*Geometric data represents the shape and appearance of 3D objects. This includes vertices, edges, faces, curves, surfaces, and points that define the object’s form
*Transformation matrices hold data used to transform those objects properties

geometric components

*A scene graph or similar system (like a localized entity system) stores and organizes the geometric data of each model component individually, along with their spatial and semantic relationships.This organization allows the code to draw particular geometric components in isolation, enabling you to select, inspect, and manipulate individual parts of a model rather than just rendering the whole scene as a flat image.The final rendered image you see on screen is the result of composing many such individual components, transformed and drawn many times per second to create a smooth 3D view.

running frames continuously simulates motion

*moving around in a 3d world requires a transformation of the camera over time.The rendering process redraws the entire scene relative to the camera’s current transform every frame (tick), producing the final 2D image on the screen.

*When you move or transform the camera, the scene graph is traversed each frame during rendering (renderer.render(scene, camera)), applying all object and camera transformations to compute their positions relative to the new camera viewpoint.This traversal and hierarchical transformation propagation is what allows the renderer to "bubble" the effects of camera movement through the scene, updating the final rendered image accordingly.Without the scene graph, you would need to manually track and apply all transformations and redraw calls, which is complex and error-prone.

*Without a Virtual DOM, using vanilla JavaScript means you must manually update the real DOM whenever something changes. This can be tedious and error-prone because you have to know exactly which elements to update, and other parts of the DOM won’t automatically react or update relative to those changes.

*so its the same for 3d.without a scene graph,if i move the camera for example,i also have to manually update all the other elements relative to it so that they too can be rerendered so the better way is to use a data structure over this layer and a runtime that lookups and manages this structure.so the app or game doesnt make direct graphic instructions but one on the this structure which makes it easy for some runtime to apply transformations across all elements relative to another when that element transforms

*a scene graph in 3D acts like a reactive hierarchical data structure that makes modeling relationships between objects natural and efficient, because transformations and updates propagate automatically through the graph.

camera is a view into the image

models,nodes

transformations

render loop

attatching event listeners to the target

*tabIndex controls focus levels
*z-index controls overlay levels

transformation variable mutation
continuously rendered function--uses a formula

the animation loop should be the only place where transformations are actually applied centralizing state and smoothing out transformations ensuring that they are applied predictably if multiple event listeners transform the model abruptly

It’s essentially an “update pattern” or “state interpolation pattern”, where you maintain a target state as a separate variable and have a dedicated update function that applies smooth transitions (interpolations) from the current state to the target state each frame.

direct transformation--uses formula on every loop
target transformation pattern

*the fundamental transformations
scaling
translation
rotation

*Weighting is not a transformation itself but rather a scalar value that controls the influence or strength of a transformation or effect.

*so the pattern most games use for transforming objects is to modify a target transformation and in a single function,they use the difference between the two states to transform the object and applying additional things like weights such as speed.Physical quantities like speed, force, acceleration, or damping act as these weight variables that modulate the transformation updates, making motion feel natural and physically plausible.

a model is an object composed of a material/tecture applied to a geometry construct

*an AmbientLight in Three.js produces light that illuminates all objects equally from every direction without casting shadows or having a specific direction. while a directional light casts rays of light at a particular direction

keybinds to transformations
Orbit controls

*so what im experiencing is a conflicting transformation which happens when two different parts of an app update the transformations of a single object themselves which will almost always happen out of sync.my keybinds dont conflict with themselves because i used the target transformation pattern but orbit controls is doing its own transformations on my camera

clumsy rotation
natural rotation

euler based rotation
quaternion-based rotation
clamp the rotation
rotate independent 3d object 
nesting the camera 
YXZ rotation
applying quaternion vector

rotating while moving

*Yaw is the horizontal rotation (looking left and right), which is a rotation around the Y axis in Three.js’s coordinate system.
*Pitch is the vertical rotation (looking up and down), which is a rotation around the X axis.

*so yaw is a horizontal rotation but rotates around its cross axis,y/vertical axis

*so the add method in three js is to nest a three js object in another

*Rotating the camera directly (modifying camera.rotation or camera.quaternion) changes the camera’s orientation in world space every frame. This can sometimes cause complex recalculations in the rendering pipeline, especially if done abruptly or with conflicting inputs, potentially leading to jitter or performance hiccups

*The camera is a child of pitchObject, which is itself a child of yawObject.When you rotate yawObject (around the Y axis), it rotates the entire subtree, including pitchObject and the camera.Because the camera inherits transformations from its parents, rotating yawObject effectively rotates the camera horizontally (yaw).

*so to rotate an object horizontally to look left or right,i actually rotate horizontally but the rotation orbits the y-axis which makes the object remain in its posture while rotating it to look up and down will orbit the rotation about the x-axis thats why yaw is aplied at the y-axis but controls horizontal rotation and vice versa for pitch

*so directly translating the camera is possible because it just needs to redraw the scene relative to how close or far they are from the camera but rotating the camera causes complex recalculations as the renderer has to redraw the scene according to the new orientation which can cause clumsy rotations if done abruptly but by nesting the camera in a normal 3d object which has more controlled rotations,i can effectively rotate the 3d object only and the camera being part of that object rotates along with it without it having to rotate by itself.so the renderer still redraws the scene but it doesnt have to handle calculating camera rotations before doing so.and by separating the 3d object responsible for rotations into separate 3d objects,i can prevent unexpected rotations/gimbal lock which happens when the pitch and yaw rotations of the same object interfere but to treat it as a single unit again,i nest one of them inside the other mmaking one rotation always relative to the other.but the pitch object is preferrable as it ensures up/down movement stays aligned with the camera’s horizontal facing.so we clamp the pitch not to prevent conflict but to prevent the camera from rotating unbounded in the y-axis which can cause an unnatural upside down rotation.This ensures the camera behaves like a real-world head or eye that can look up and down but not spin completely over itself.

*so yaw rotation that controls facing left and right which orbits the y-axis should come first because naturally,we turn to look left or right before looking up or down.so doing yaw first is better and more natural so that looking up or down is relative to us looking left or right

*Three js handles rotations as quaternions internally on all 3d objects but exposing a simple euler rotation interface.so separating them even though quaternions already prevents gimbal locks helps me to focus on rotating one object focusing on one axis at a time which allows for isolated and simplified logic for each of the axis

*when i rotate the camera using the yaw and pitch objects,i rotate it orientation in the world space but when i move it,i move it along the world axis not from the camera's local axis which comes from its own different point of view as a result of rotation

*so its better to use the vector datatype to store x,y,z cords data for points or transformations than separate individual numerical data types.

*so add on a vector performs a vector addition while for 3d objects,it nests them under its hierarchy.using a vector allows me to leverage this over arithmetic

*so lerp is a vector operation that updates the current data to the target using a weight.its the cleaner form of calculating the delta and weighting it myself using arithmetic on multiple numerical types representing the cords

*so apply quaternion is a vector operation that updates the cords data of a vector to align with that of another which in this case is used to align the rotation of the object with that of the camera so that ist translation isnt on the fixed world axis

*A quaternion represents a rotation in 3D space, encoding an axis and an angle of rotation.

*so the cords of a vector rep as (x,y,z) only hold point or direction data.the rotation data is rep separately and internally as quaternion but i can interface with the rotation as arithmetic values(Euler angles).Quaternions can be applied to vectors to rotate them, transforming their coordinates accordingly.

*so for both position,translation and rotation data,i can manually manage numeric values and use them to update the corresponding properties of the objects but using a vector data structure for the postition and translation while an euler object for rotation is better


*so i have to initialize the vectrs even afer defining them to align them with the camera’s current actual position and rotation in the scene.This ensures that the target values start synchronized with the camera’s real state, preventing sudden jumps or erratic behavior when you begin moving or rotating the camera.Without this initialization, targetPosition and targetRotation would start at default values (often zero vectors or zero Euler angles), which likely differ from the camera’s current transform.

*pointer lock behavior depends on browser and OS implementations, and certain input combinations—like simultaneous key presses and trackpad gestures—can cause pointer lock to be lost or mousemove events to be suppressed

*Pointer Lock API delivers raw device input (movementX, movementY), which reflects the actual physical mouse or trackpad movement.
*Programmatic rotation (e.g., setting Euler angles or quaternions) can simulate rotation visually but does not generate real pointer lock mousemove events.
*Browsers do not allow synthetic mousemove events to emulate pointer lock input for security and usability reasons.
*Without real pointer lock input, you lose the continuous, unrestricted relative movement that pointer lock provides (e.g., unlimited yaw rotation without cursor hitting screen edges).

^ill load plugins as ts src files directly cuz of security reasons and compile it to wasm before the app starts if possible to do securely to get the best performance.the wasm files can be temprorary and deleted immediately once the app closes.

*so in first person,the camera controls the movement and the player moves relative to the player but in thirs person,the model controls the movement and the camera orbits the player moving relative to it