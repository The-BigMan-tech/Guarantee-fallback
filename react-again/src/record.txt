a thunk is a function that returns another function instead of a state.Its used for performing side effects

ill use a progressive optimization technique where the core functionality is written in js at first using thunks and the code in the thunks is gradually replaced with rust calls

decoupling the implementation from the application

*local file manager
*cloud file manager

redux thunks for client side processing with side effects
rtk query for communicating with server side processing

redux state
reducers manages the state of the app
thunks orchestrates state and call side effect functions
side effect functions/utilities are separate and decoupled from the thunks

components dispatch thunks

undefined: Typically indicates that a variable has not been initialized. This can lead to ambiguous situations where itâ€™s unclear whether the absence of a value is intentional or accidental.
using null union ensures that a value can never be accidentally undefined

switch on union for safety similar to match enum case in rust

variables with no values leads to ambiguity about whether its intentional or by accident.so using null where possible is better so that you can clear this ambiguity and save productivity time from false bug hunting


async thunks and create async thunks

thunks dont return values.just dispatch actions

^react,tauri,pouch db,tailwind,daisy ui,redux,chart js,react-toastify,uuid,throttle-debounce,fuse js,jotai,heap js,flexsearch
wasm

hello pangea dnd,react router,framer motion,gsap,three js,
next js,auth js,paystack,prisma,postgres sql,mongoose,mongo db
formik,zod
web3forms
backend as a service or a nest js server

^uno edit funk download
^Conquest edit,invincible season 3 edit

relative ui sizes only makes layout responsive and not content 

client side apps--tauri with react,vue,svelte,rust for performance critical parts
server side apps--tauri wrapper,next js,nuxt js,sveltekit
client side with server side functionality---client side app with rtk query,Backend as a service/nest js server
server side app with client side functionality----server side app but with (redux,client components,pouch db)

implementation,interface,controller
database,file system,app state


TODO:download some assets
TODO:finish the layout of the ui

Todo:Art styles
3d illustration art
neumorphism and skeuomorphism art
minimal ui art
vibrant
animated
infographics
svg art
lottie animation

todo:resizable panels

3d illustration ai generator

fixed units---requires manual responsiveness,is faster:%,vh/vw,rem
relative units--constant calculations makes it automatic,slows perfromance:px

relative unit for layout
fixed units for content

safe assignment


*so components gets rerendered across updates and only app state is saved across rerenders.return values from function calls arent saved but rather recalled on rerender and since uniqueid isnt a pure function because of the unpredictability of output,the previous ids get lost on rerenders and thus the need to cache their iutputs using useMemo is required

*i could have made the conditional class logic inline by using the ternary operator but doing so will become catastrophic as the component grows because i will have to check each element that manages their own classes that they apply.moving it to a function that an element calls even if its one element makes it easier to reason about.the jsx shouldnt make any decisions on the app state or which classes to apply.they should just render ui.the only decision it makes is on what ui to render based on a condition it doesnt even check but one returned by a function.

*sub functions are the functions within the component function that handles component state management logic.They often read from or modify component state without directly embedding that logic in the JSX.sub functions only make decisions based on the component state but when it comes to the app state or global state,they only dispatch thunks that handle that logic and they dont decide which thunks to dispatch conditionally because they will be influencing the global state.The subfunction determines which ui to render and the jsx follows.

*jsx are no decision makers .the decision making is by sub functions even what to render is by a subfunction so a jsx rendering something conditionally is as directed by a subfunction.the subfunction will return its decision as either true or false and the jsx obeys.

*operations are better centralized than decentralized,have only one responsibility
*components are functions that return jsx.

*reusable styles will require a css file and not necessarily a style component or a string variable
*reusabel jsx can be done with components and props

*useMemo a hook for memoizing values in components.It helps optimize performance by recalculating values only when their dependencies change.Ideal for expensive calculations or derived data that you want to cache during component re-renders or for retaining the values of impure functions like a unique id generator

*React.memo is a higher-order component that memoizes the entire component. It prevents re-renders of the component if its props have not changed.Ideal for optimizing functional components that do not depend on internal state and only rely on props for rendering.stateless child components can be memoized

*regular function defintions for reusable logic and arrow functions for callbacks in hof
*another use case of arrow functions i found is that it looks cleaner for components that accepts props.they are also used for type annotating functions in interfaces

*most of the time,i dont use props because state can do everything i need but i think the ideal use case of props is for pure components.that is components that always return the same output from a given prop.unlike their parents,they dont have subfunctions to make any decision for themselves.they just take the sub functions of their parents and call them where appropriate non conditionally.They are mostly used to prevent rewriting jsx across a component.If they do make decisions,its only based on their own state.they dont make decisions about their parent state so they shouldnt directly have setters but rather subfunctions of the parents that do the decision when it comes to the parent state

*arrow functions,props and memoization for child components

*Child components--they are the same but they dont affect the state of their parents directly but rather use wrappers around setters provided by the parent and it should be changed non conditionally


^fs,path and os,win api
the os module is used to request general os services
win api is specifically for window services

todo:safe mode--changes to the fs through the app doesnt directly affect the real fs.this is to ensure the app safely access the fs.It will require a diffing algorithm and diff files that are loaded in memory.any attempt to write a file,writes to a diff file

design merging
primary-complementary design


lasnguage service provider--intellisense
linter---real time analysis
lexer---breaks the code into tokens
parser---constructs the ast from tokens
semantic analyzer--checks if the ast is syntatically correct
ir code generator
ir optimizer
asm code genrator
assembler
binary

?does php compile to html

software architecture
design architecture
algorithm design

*---------------------------------------------------------------------------------
implementation,utiilities--a bunch of tauri plugins
safe interface
selectors,reducers
thunks
components--state getter and setter,sub functions and jsx
Child components

higher order components
toast component

custom hooks
higher order thunks
cache utility--normal caching and ahead of time caching
app.tsx for setting up the app on startup using useEffects.like opening a tab on startup,caching specific folders ahead of time and deciding how the ui responds to aot caching
throttlers

jotai atoms--for unserializable data
*------------------------------------------------------------------------------------------

app state is global,component state is local

stateful child,
stateless child

custom hooks for reusable component logic
stateless child components for optimization and reusable jsx

anything that should cuase it to render when memoizing,should be passed a prop and checked for changes

ui framework compatibility tools-mitosis,astro

tauri uses wry as the webview library

todo:how to embed external binaries like node js into tauri
calling rust from frontend
calling frontend from rust
bidirectional communication

delayed runtime error--error-prone code may not immediately cause runtime errors till they are called.This is because writing error-prone code doesnt cause anything till you use them at runtime.

^with wasm,you can interop any programming language with js.so i will use tauri or was to bridge rust with js

unsafe but protected like for a new unsafe block,a new process is spawned and if it crashes,the program skips it

todo:vfs feature that uses a diffing algorithm
todo:virtual folder

frontend dev--content but exhaustive
backend dev--content but repetitive
cyber security--new content generated every day because of emerging threats
game dev--you can create any thing you can imagine as long as you can code it
data science programmer--new content because data changes per business
scientific programmer
cloud developer

the reason we use state on react instead of local variables for storing data is because components i react are functions and functions are stateless so when they rerender,the variable information gets reset but using state from a state hook ensures that the value persists across rerenders

you can return jsx with or without () because () is for grouping data but since jsx only has a single parent,there is no difference in using it or not but using () allows you to indent it below the return.

for a toast component that notifies the app on an error,define th error in the component and bind it to a use effect that calls the toast function to show the error but you should define the toast function in the use effect hook or it will always cause a rerender.you can define it outside of the hook but it will be more complex as you have to use useCallback to prevent unnecessary rerenders

everything used in a use effect hook thats outside of its definition must be a dependency so that it accurately reflects the current data.if a variable isnt tied to rendering of the hook logically,its better to define it within th euse effec hook to prevent it from being a dependency

*ui configuration to component with set attributes

toast emmitter is a local change while toast container is a global change to all toasts

*The toast component works by listening to a variable on the app state that keeps errors,infos or messages and when it changes,it uses a useEffect hook to respond to it.The problem is that the same error message wont popup again after it does once even if the error occurs twice.this is because the same error message means the state of the error didnt change for the useEffect to listen to but by using an id property that has its value generated by a unique id function at runtime,the same errors will popup twice even though the messages are the same because their ids are different.This is for incase someone spams something that gets an error and they need to know it gets an error each time and when they want to redo the same command for refreshing the component

?errors can be handled,transported or both

objects are for centralizing related data into a single variable

because the ids are supposed to be unique even for the same message,it has to be recomputed unlike elements in react list components that requires caching for the components to be properly tracked

*CODE SAFETY
type safety,memory safety,runtime safety,thread safety,access safety like mut refs
logic safety,external safety

difference between it will not x and it can never x
notes in my jotter
difference between safety and security
builder mindset vs the engineer mindset

even though thunks look like regular functions,they can only be called by dispatchong them even by other thunks


*error transports---log files,console,toasts
*safe interface,centralized error management,transport layer
*capturing the error,handling the error,shipping the error

*compile time errors,runtime errors,logic errors

setup--scaffold your project,a clear architecture,make small of the backend to think of your app in terms of functionality,make small of the ui to think of the app in terms of user interactivity.use that mindset to finish setting up the framework of the backend.so that when you go back to the ui,you can easily add features to the backend and connect your ui to it.the backend may not be finished at this stage just enough for you to have a structure of what you are building

^vscode tabgroups

*the thunks can also perform file system operations on the app level by leveraging other thunks

updating a state thats also a dependency in a use effect will cause an infinite call to that useEffect.You will want to break down that useEffect in two separate ones to prevent this issue while still retaining the desired intention:One for setting the state and another for reading it.In general,useEffect blocks shouldnt be large so that elements that dont logically depend on its dependencies wont render unnecsessarily.its best to use many use effect blocks that are just small enough to hold dependents than a big one.

*im storing cache data as part of the app state in memory and its only managed by thunks not components.components dont know the cache exxists

bitiwse,unioon and logical or

push,pop
unshift,shift
indexing for replacing
splice for inserting,deleting or replacing at any index

freeze the ui on loading

the asynchronous nature of the openInApp thunk is the reason why i dont see the results of the dir operation as soon as i opened it in the app but until i open another one even when i read the data only after performing an operation on it.

*The caching technique I used is not with local storage or pouch db but an in memory approach.this means that the cache is not stored on the disk and as such,it clears once the app closes.The cache is a property on the app state that's an array of objects holding two properties;the path to the folder that's being cached and the data which in this case,is an fs node.The cache grows on app usage where data is pushed as the app is used making repetitive patterns load much faster.This prevents space from being used on the disk and its  easy to implement.The cache though will need to be limited where it removes the first element as it exceeds it's threshold so that it conserve memory usage  but since the fs node type I made doesn't store the files content,only data like ext,path,icon and other metadata,I can make dozens of them without affecting memory.The content is loaded on demand from a function at runtime given that the path is provided.So regardless of the file size,all fs node types take almost the same memory.The length of the path is the only difference in memory which is subtle.So my cache can go to 40-50 of these and this is O1 space complexity too at that because the cache has a fixed size.My previous projects,I made both the thinks and components handle caching which raised complexity but now,the thunk manages caching and components will use the app state comfortably without even knowing of they are working with cached data or not.The thunk does this by setting the app state they listen to to the cached version till it's done loading where it will st the app state they listen to the version that was just loaded.Thjs preserves the user experience.When you add data to the cache,it will also check if it exists and if so,it will update it.The thunk will only cache after the it updated the ui allowing caching to happen in the background

I can cache 50 of fs nodes in memory cuz its just metadata really

cache limiting by number
cache limiting by memory

so even though the cached folders is fixed,the fsnodes in each folder can vary but thats not a problem since the fsnodes only contain metadata so having many of them isnt a significant memory hog.

metadata caching
full data caching

clearing the ui and transporting the error is better than crashing or exiting the process

freezing the ui

so freezing the ui when reflecting cached data and while relaying to the users that the new data is reloading. is safe because the cached data may not reflect the new data so users may interact with an unstable state and it prevents unintended actions from layout shifts upon reloading.so it means that the cached data is there for the ux and immediate feedback so that the folder wont look empty all the time they click on it but the time of interactivity is still the same.



*you cannot compare objects or arrays in js since they are done by ref not value

setTimeOut/interval,Promises,async/await are ways of async js

*states in react only change when rendered so if they need update based on state changes,they have to be updted in a useEffect but if they use a global app state,they stay subscribed and update when the app state changes.But if a state that depends on another state thats subscribed to the apps global state,it wont update when the subscribed state responds to global state changes.it has to be listened to on a useEffect which manually updates the local state thats connected to the subscribed global state
 
*my app only loads the immediate children of a dir and only loads the individual content when requested

I will have done ahead of time caching where all child folders of an opened folder are cached but since i only load one generation of child folders at a time,i cant cache the child folders because the next generation of child folders under those ones havent been generated yet meaning i have to do that before caching the first generation child folders.This is possible but not feasible and will cost runtime even though it happens in the background as some child folders wont be included in the cache when the cache reaches its threshold making it to use extra runtime for nothing and the cache limit also means it will be unpredicatable to know which child folder remains in the cache.the purpose of ahead of time caching was to improve ux by caching other folders when a foder is opened so that users dont see loading for them but it isnt feasible.

so if i want to do ahead of time caching,it has to be selective and only on app startup.selective ahead-of-time caching should definitely be optimized over time based on user behavior and performance metrics


the way my caching mechanism is means that there is no different between it and a skeleton in the sense that it isnt interactive except that it shows more information than it.the cache ahead of time worked except that there has to be a decent amount of time before everyhting can be cached so the user seeing a cached downloads for example when clicking it for the first time depends on whether he was else where long enough.but i guess ill leave it like that.its better than not caching anything ahead of time.i can decide to prevent interactions till everything is cached ahead of time but it spoils the ux as users want to work with something as sooon as they see it loaded an they wont like long loading times

the name cacheAheadoftime is by its context or use case not that it caches ahead of time.its a normal caching function but how its used qualifies it for that name.

debouncing prevents unnecessary caching attempts.debouncing made the ahead of time caching faster than when it didnt have it because caching the home tabs can be skipped by the app when the users rapidly switch between tabs on app startup.Also,the thunk only caches one tab ahead of time so its in the app.tsx file that ill decide the order in which they will be dispathed.By making the thunk to only cache one at a time,i can configure the order later and prevent all of them to be in one big promise waiting to be resolved at the same time but instead in smaller promises.


the pattern where the tabs are cached ahead of time sequentially automatically prevents the need for a throttle because opendir runs only after all the cachedaheadoftime functions because open dir awaits twice midway while the others only await once meaning that the app wont show the home tab till all the other tabs are cached ahead of time even though i placed openhome first.opendirinapp already caches the tab for me if it isnt being cached by the aheadoftime function so everything sorts out.So when i open the app,it starts at the home page and loads but while its loading that,the other tabs will be cached unlike where i did everything all at once under thunk where even after the home page loads,some may still be under caching.The only thing now is to ensure that the app remains uninteractive till the home page loads.

its better to break async tasks that repeat tasks into independent async tasks so that they can be managed effectively

so not only is the caching ahead of time but just in time meaning that once the home page loads,everything that was cached ahead of time is guaranteed to be cached.the feature where the other tabs are cached as soon as the home tab loads created itself out of the fact that the caching operations are async and separate

The opendir function skips over caching tabs that have already been cached ahead of time.this is to prevent switching between tabs to cause unnecessary attempts to cache when the tabs are still being cached ahead of time.but it will be a problem as the cache wont update on subsequent folder updates so i made each aheadofcache function set a state that indicates to the app when it has finished.This is my way of invalidating the cache of the ahead of time caching

he cache is limited by instances of 20 and the individual fsNodes array of each instance is limited to a 100

i just realized that prints in async blocks may not show in the order they were executed.so they are a bit unreliable i believe.

there are two types of caching.just in time caching or ahead of time caching.jit caching costs runtime but no startup time while aot caching is vice versa.jit caching should be used to cache evrything because it ensures that only accessed data is cached but aot caching can be used to improve ux but it must be selective so that it doesnt take too much startup time or waste cpu and memory resources and must be async.

jit caching is defined and initiated at the thunk level.

the aot caching state is read by the app and used by the app to control the freezing of the ui and as such,it isnt manged by a thunk.the aot caching is defined at the thunk level but its initiated by the app and the app component that decides how the ui should respond to it


*there is css,
*sass 
*tailwind css as in with @apply directive.its css syntax but for tailwind
*tailwind themes as aliases
*styled component
*tailwind styled components

*prolog is for old school ai

if you dont invalidate a memoized component,it will see an older version of a state and not behave as expected

*just in time/on demand/lazy
*ahead of time/eager

mine freezes the entire ui till everything is cached ahead of time.i can do a progressive aot caching where users can access data that has been aot cached

jit caching is best for unpredictable access patterns and frequently changing data
aot caching is best for predictable patterns and very stable data

AOT is best for
Data that is accessed very frequently.
Data that is critical for initial application functionality.
Data that is relatively static.

*so the reason why my component doesnt reliably call the cleanup function when it exits is not because of js or react but because its impossible to guarantee that the app will have a clean exit as many circumstances can prevent the app from closing in a stable state.task manager is an example of this,it forcefully exits a process but closing the window normally,gives the app the chance to cleanup and thats why force quit is faster than normal quitting.React's useEffect cleanup functions are designed to handle the component unmounting or when dependencies change. They are not intended to be a guaranteed "on-application-exit" mechanism

this wasnt immediately obvious because theoretically,it will provide a reliable exit and js cant warn the programmer about all possible operations that will unreliably run when placed in a use effect.so useeffect is for managing app resources that arent critical to the functionality of the app.if i need to persist data when the app exits,i have to create the solution myself.

so external code safety can never be guaranteed because its the fundamental limit of computation as described by the halting problem.

retry mechanism is a way to solve transient errors which are errors that only fail on a lack of certain conditions.fallback,rollback,fault tolerance,indempotency means that performing the same operation many times is the same as performing it once.its like the safety of a pure function but in a function that does side effects

*if i want to reliably persist data,i can use a polling mechanism,retry mechanism or checkpointing for persisting data.

*so making a requirement like only once for a design architecture where the app only stores the data when the app is not in use is a bold requirement that makes it complex to guarantee.

*so at-least once persistence is better than only-once persistence.so the limits of computation affects design decisions because under ideal conditions,one can design his app to only persist the data once the app is closed.In the real world, systems are subject to failures. "At-least-once" acknowledges this reality and prioritizes ensuring the data eventually gets persisted, even if it means potential duplicates.This is why persistent operations must be indemptotent and a retry mechanism must be implemented

perfect reliability and predictability isnt possible in computation

only-once is the ideal condition and at-least once is the real condition

theoretical designs mostly consider ideal conditions then it will convert to a practical design because it now starts to consider real world conditions

app level startup operations more reliable to execute than on exit operations since the app only starts under some normal conditions that can reliably execute the startup function.The phrase "normal conditions" is crucial here. Startup operations are designed to execute under normal conditions. The application expects certain resources to be available, certain dependencies to be present, and certain configuration settings to be valid. If these conditions are not met, the startup operations may fail.


todo:checking on exact matches of strings is error prone without consistency.change all of those to distinct states.But it will do for now.Using string transformations like decapitalizing the string and trimming off white spaces can make the match less sensitive and thus,less error prone.



items in the recent folder are links

incremental processing

so i can get predictable incremental processing unlike removing the await blocks

Predictability: The order in which the promises are initiated is predictable. The order in which they resolve might not be, but you can handle that.

*so returning promises creates predictable async operations but removing await keyword on unresolved data will give unpredictable async operations.it may not behave as intended and is error prone.You're then trying to use data that might not be available yet, leading to errors and unpredictable behavior.

*im returning an array of fsnodes promises instead of processing it all at once before returning or rather than using a generator for them because its easier to write and understand than an async generator.

*incremental processing shows more progress than all in one processing but in my specific use case,its slower even though it incrementally shows progress because each time i update the ui when a promise resolves,the ui has to take its time to update rather than the previous one where it updates all in one go.so i will only use it for the home tab which is the first tab that loads when you open the manager.so that the users can see a visual indicator that the files are loading while the app is caching some tabs ahead of time.to prevent the number of ui updates,i will batch five then push.

*one can argue that doing ahead of time caching and incremental processing only cost additional runtime but im only doing incremental processing on the home tab to leave a better user experience.even if its slower than all at once,the user can track progress as the app opens instead of a blank slate that just gets filled all of a sudden.it builds reliability in the app and then,the other tabs can then be loaded all at once,it will look like the app just became suddenly faster after the initial load further building reliability in the app.the caching also also ensures that users see a cached page thats showing a load in progress since the app only loads one folder at a time so that they dont see a blank page that saying loading.so ahead of time caching for the six tabs on the sidebar ensures that the user always sees progress.so this specific scenario is an example of when user experience is more important than runtime.the rest of the app can do all at once loading.

*You're tailoring your loading strategy to the specific context of each tab. The home tab prioritizes initial impression, while subsequent tabs prioritize efficiency.

*so in general,comparing arrays by using a deep comparison is highly inefficient especially if the condition check is only used frequently as the time complexity is O(n) but the direct approach uses logical chainshas with a time complexity of O(1) making it extremely fast and independent of the size of the array.so comparing arrays based on logical chains is more efficient cpu wise than deep comparison but the deep comparison can be potentially easier to read than the logical chain and can apply to a wide variety of arrays than logical chains that are tightly coupled to a particular condition

*so wrapping a function asynchronously through a promise or async marker doesnt make the operation itself async only the result of the operation is treated as async and any other operations in that function that work with async results

*so async and promise only defer the result not the operations leading to the result but the only operation that gets deferred is one that depends on a deferred result.The operation still runs when and where it would have without the async/await or Promise, but the code that depends on the result of that operation is deferred.

*IO operations like network requests,disk and databse operations are blocking and expensive to perform.There are three ways of doing them;onEveryChange,at-least once,at-most once and only-once.only-once is feasible on startup but unreliably on exit.it is done assuming ideal conditions but in reality,at-least once and onChange are the only ones that are computationally feasible but onChange is highly inefficient because it will block the main thread.if i make an app that launches a network request on every button click or the app wont work,the app will have an extremely poor performance because its heavily reliant on io operations and performing them onChange is runtime expensive and the app cant afford to make it async because it will lag on interactions.An app should use io operations but not heavily reliant on them.The at-least once approach is the most practical approach to handling io operations.It ensures that an app does an io operation at least once because its dangerous to make it run only once as external factors can break this guarantee especially on exit operations.It can be done in many ways such as a retry mechanism,based on frequency or asynchronously.

?service workers and web workers

*so a side effect is when a function modifies a parameter in place/mutation or performs an io operation.Pure functions are more predictable because they dont have any side effect.if a function must perform a parameter side effect,the programmer must ensure that its clear and concise to preserve predictability but if the function performs an io side effect,it must be indempotent.its an io side effect function that intends to act like a pure function by ensuring the overall state change caused by the function is the same, regardless of how many times it's called with the same input.

*so an indempotent result is that:if i attempt to perform x multiple times with the same input,it only performs x once and does nothing on subsequent calls

^i want app.tsx to be responsible for persisting the inmemory cache and loading it again to memory and caching objects ahead of time.


*Local storage has a limited storage capacity (typically around 5-10 MB per origin). If your cache grows too large, you could exceed this limit, leading to data loss or errors.Writing to local storage too frequently can impact performance, especially on low-end devices.Reading a large cache from local storage on app load can also cause a delay.

*using a blind timer like setinterval is unsafe.it can perform unnecessary writes even if the data hasnt changed which wastes cpu resources.If the timer takes longer than programmed which can be influenced by external conditions like a slow device,the next setInterval execution will start before the previous one finishes. This can lead to race conditions, data corruption, or performance degradation.


*The setInterval timer itself has a very minimal performance impact. The overhead of setting up and maintaining the timer is negligible. The real performance bottleneck is the localStorage.setItem() operation, which is synchronous and can block the main thread.


A time window,time interval,timer


*throttling and deboucning are both techniques used to limit the frequency of function calls based on a time window to improve performance except that throttling only calls the first or last function call to arrive within a time window while debouncing only calls a function after the function hasnt been called for a time period/a period of inactivity

*leading throttling is that only the first call to arrive in the time window is executed while trailing is that the last call to arrive within the time period is called.The time window reset again to 0 to allow for new calls to arrive.

*Throttling and debouncing can indirectly increase the likelihood of no race conditions.However, this is a side effect, not the primary goal. Debouncing doesn't provide any guarantees about the order in which requests are processed or responses are received. It just reduces the number of requests.Debouncing is a rate-limiting technique, not a synchronization mechanism.Throttling and debouncing dont know anything about the app data so they arent indempotent

*The difference between at-most once and only-once is that at-most once only calls a function once within a time window ensuring that other calls during that time are ignored while only-once calls the function once within the app's life cycle.

*the main issue of putting a throttle in a useEffect that listens to data that changes on every button click for example is often that the throttled function is being recreated on every state change. This defeats the purpose of throttling.

at-least once uses an interval
at-most once uses throttle or debouncing.at most once prevents unnecessary writes.

^async database

*reactive io + throttling = at-most once.so store data to local storage is throttled meaning that on every call to it which happens on every add to cache,only one will be executed during that time window.It will be more effective if i increase the time window so that if a user clicks a button like five times,it only calls the store function once and since the cache needs to be up to date,it has to be trailing throttling.A shorter time window will result in more frequent updates to localStorage, while a longer time window will reduce the frequency but potentially lead to slightly older data being stored

*so if i used an interval like set interval,it will be an at-least once approach like store the cache to storage every 10 seconds but its more fragile because it makes an assumption that the cache may be updated within that time which cant be guaranteed because of different user behaviour and it will waste resources as there may be unnecessary writes

*The types of io operation techniques are only-once,at-least once,at-most once,reactive io,manual io
*so only-once is the ideal io operation techique where the io operation is performed only when the app starts or exits.startups are feasible but work under assumptions while exit io operations can only work under absolute ideal conditions.At-least once,at-most once and on-every-change are the only practical approaches.at least once is better for retry io mechanisms like constantly trying a request till it succeeds but it requires that the functions are indempotent and its fragile for operations that depends on data changes,at-most once is best for io operations that depend on data changes.It combines reactive io with rate limiting mechanisms like throttling or rate counting.Reactive io deivers io operations on changes to the app state and its the easy way out but highly discouraged to be heavily relied on because it significantly spoils performance.Manual io doesnt face the challenges of others because it shifts the responsibility to the user but it risks data loss if the app doesnt close gracefully,it creates an inconvenience for the user and it isnt best for background operations.on-data-change is feasible if the operation isnt meant to be frequently done and its lightweight.many applications still require manual io for a lot of functionality like deleting a file when a user does it.

^Extra notes on io
*the implementation of the trigger is what truly reveals the reliance
*Io operations have this complexity because its outside the control of the codebase

^IO optimization
*batching,throttling,debouncing and async io are optimization techniques on io operations not delivery triggers or semantics.because without them,it means that every trigger to an io operation will block the main thread to perform the io operation
*Batching is based on count while debouncing and throttling is based on time

*Batching is not a way of delivering io like reactive,manual or at-most once.its just a way of handling how many io operations are done at a time.Its an optimization technique

*Throttling is not necessary to achive at most once.its naturally at most once but throttling is an optimization mechanism

*Debouncing:you know it already

*Asynchronous io operations

^IO delivery trigger:What triggers the io operation
*manual io:Done in response to user events
*reactive io:Done in response to changes in the app's state.
*automatic io:Done automatically in the background like a retry mechanism


^IO delivery semantics/guarantees:What are the guarantees that it will perform the io operation
*Exactly-once:The ideal approach where the io operation is done exactly once.It guarantees that the io operation will be done and only done once.It combines the guarantees of at-most and at-least once approach or the other way around that at-least once and at-most once approach are techniques that  only implement one guarantee because onl-once relies on assumptions and ideal conditions that rent just computationally feasible
*Here,x = 1

*At-most once io means that the operation is either done once or it isnt done at all.There is no guarantee that it will be done.But it guarantees the same operation wont be performed twice to prevent unnecessary io operations.It isnt reliable and cant be accepted if data loss is unacceptable.At-most-once is the default behavior when you don't have any retry or acknowledgement mechanisms like atomicity
*Here,x = 0 or x = 1

*at-least once:The io operation will be attempted multiple times till it succeeds.It ensures that an io operation will be done but doesnt guarantee the number of times it will be done which can lead to duplication if the operations arent indempotent and even if they are indempotent,it can waste runtime unnecessarily
*Here, x >= 1

*Effectively-once:approach is a practical attempt to do exactly-once and acknowleding real world cases where it may fail.its a real world approximation.Because of its complexity,its not used for applications but sophisticated systems like databases.
*Here,x ~= 1



^How io trigger choice affects application dependency on the io
*Reactive I/O (Highest Potential Dependency):This implies the highest dependency because the application's core state transitions are directly tied to the success of I/O. If the I/O fails, the application's behavior is immediately affected.
*Its for io operations that are valuable and critical.should be used judiciously.It has a high trigger frequency

*Manual I/O (Moderate Dependency): This implies a moderate dependency. The application functions independently of I/O until the user explicitly requests persistence. Data loss is somewhat acceptable otherwise the responsibility wont be shifted to the user, but it's still important enough to provide a mechanism for saving.
*Its best for io operations that are valuable but not critical.It has a moderate trigger frequency

*Automatic I/O (Variable Dependency):The implementaion decides how much the app relies on it
*Whether its valuable,critical or not depends on the implementation.It also has a variable trigger frequency.You can use intervals or debouncing to do this.so even though reactive and manual io patterns can solve cases of critical and non-critical io operations,automatic io is done where conveniency is needed or if the io doesnt have any specific trigger rules that it just fits perfectly in the background.

*so the trigger pattern is about how reliant your app is on the io operation while delivery is about the level of guarantee you expect that an io operation will perform and perform correctly.The trigger pattern defines when the io operation will be initiated.



consistently audit code that uses relatively old libraries.dont use absolutely old libraries

*i can only throttle a normal function and not a thunk.Because the throttle is only controlling the outer function, the inner function is dispatched every time throttleStoreCache() is called (within the throttling window).You need to throttle the execution of the thunk, not the creation of it

*so type assertion is to relax the ts compiler in situations where we are sure of the type but its too complex for ts to know that. You use them when you have more information about the type of a value than the compiler can infer.This often happens in situations where:You're working with complex libraries or APIs where the type definitions are not perfect.You're doing some advanced type manipulation that the compiler can't follow.You're absolutely certain about the type of a value, even though the compiler is complaining but if you're wrong about the type, the compiler won't catch the error, and you might end up with runtime issues.

useMemo is for memoizing data while useCallback is for memoizing functions and React.memo is for memoizing components.They both take what the y memoize and a dependency that invalidates the memoization

*so throttle + useEffect just gives us back reactive io io but memoizing it gives us the at-most once io approach.

*so faulty type errors result when a script that doesnt officially support types is given by external declaration files which may be incorrect or out of date.This can cause false and misleading compile time errors that will take hours to figure out.declaration files provides the ts compiler with information about the types of the data used in the codebase and if the information is broken,ts will falsely flag code as type errors.The ts compiler blidly relies on the d.ts information over runtime functionality because its impossible for the compiler to analyze runtime behaviour.so the work around is to use runtime behaviour to analyze declaration files

im choosing to use spreading to the fsnodes array instead of pushing to it incrementally.it has a bigger time complexity of O(n) since O(n + 5) = O(n) as i only concatenate an array of 5 elements at a time but in exchange of faster runtime by reducing constant ui updates instead of pushing which has an O(1) time complexity but costs runtime as the ui has to respond on each update

increasing the time window of the throttle reduces the function call frequency but creates a lower chance of the function actually getting called if anything that will compromise the app should happen within that period

i made element eviction from a cache to start at the 8th index instead of the first its the same effect as shifting but it does this starting from index 8 so that the ones that are cached ahead of time will be preserved on the cache threshold.


so html already has previews i can use like text area,video and img.i can just use that.is there any other previewer i may need


video,audio,txt,pdf,<> file,word,zip,json,xml,exe,msi

^TODAY
recent folder-fixed the breadcrumbs,added support for it and cached it ahead of time
made incremental processing on the home tab
did at-most once cache serialization with throttling
preserved aot cached tabs when the cache reaches its threshold
smart reloading
file icons--ongoing
file search,debouncing
framer motion
lazy recursive file search
todo:previews--video,img,audio,text area tags,react pdf,highlight js,react player,mammoth js,three js,react window
todo:file editing on the preview editor
todo:file actions on a pane --in-state modification and io operation for significant performance
todo:file actions on right click
todo:index for in dir file actions and a transfer object for move-like file actions
todo:Changing the view layout and icon size
todo:Change it from local storage to pouch db for async database
todo:File content search
todo:Details pane
todo:disk usage progress bars
todo:responsive
todo:diffing algorithm
todo:smart automation,heavy customizations
todo:vector database search

todo:Learn search theory

react window uses virtualization which means it lazily loads the content to the viewport.

memory theory
type theory
error theory
io theory
Synchronization theory
immutability theory
indempotency theory
test theory
Concurrency data theory-stale reads and race conditions
debugging theory--step through/detail hunting debugging,dismantling debugging
reactivity and state theory
big o notation theory

*stale data theory concerns about the factors that causes the timing of read and write operations to the same data to be misalligned which causes working with inconsistent or outdated data

opens a dir to read its contents,gets the metadata and construct objects.doesnt read file content yet
file content is read on demand
since only one dir can be opended at a time,opened dirs are cached
cache instance limiting,fsnode limiting
updating the cache on loading
storing the cache using at most once and throttle approach 
selective ahead of time caching
incrmental loading
smart reloading--automatic io,background process.no need to worry about layout shifts in the sliced cached data.data will be added incrementally

grid vs table

version control--diffing algorithm
databases--effectively once,io approach

translucent tb--acryclic,clear


smart loading is composed of a rust file watcher,a bacground process that uses it and a debounce function


*im thinking of implementing smart loading in my file manager.my file manager can only read and store one dir at a time in the app state because of performance and memory but i use a cache to keep track of accessed dir data and display that whenever a user opens a dir while the app reads the dir again because of the possibility of data change in the fs between the caching and time of loading the dir.so what im thinking is that ill use a background process that can watch the file system of a dir and its only when that wathcer reports a change,that my app will reload the dir while displaying the cache data for the user to see progress,else the cached data becomes the loaded dir.no need to read it again.

*the purpose of the smart reloading is to prevent the app from reading unnecessarily from the filesystem when the cached data is up to date.without this,my app will have no way of validating if the cache is up to date with the fs and has no choice to go the safe route of rereading it while the cached data is being displayed to the user.Its going to watch only the immediate content of the dir since the cache data only has the path and data of the immediate directory not its children.the children are treated separately as their own instances in the cache to promote lazy loading.so ill have only six file watcher instances which means that its selective file watching.In case the file watcher didnt report changes for the app to reload the cache,i will have a function that runs every 10 seconds of inactivity to reload the dir of every cache.

the only reason why the cache can be invalid in my app that it requires reloading is because changes to the file system can happen outside my app.if it were only within my app which isnt possible,it wont need to have a smart loading feature.Because external changes are the issue, a file system watcher is the ideal solution. It passively monitors the file system for changes made by other processes and triggers a cache reload only when necessary.

^notify,home crate for rust

file modifications are internal changes so my app doesnt need to refresh the page.the changes will then persist to the fs

*Tauri has a bunch of plugins and i already made som myself but the best practice is to leave my custom functions the way they are as they are stable and use tauri plugins on subsequent use cases of low level functions

selective and non recursive watching the home tabs like i didi for selective ahead of time caching

*by default, Tauri restricts access to potentially dangerous commands like file watching for security reasons.

*This module prevents path traversal, not allowing parent directory accessors to be used (i.e. â€œ/usr/path/to/../fileâ€ or â€../path/to/fileâ€ paths are not allowed). Paths accessed with this API must be either relative to one of the base directories or created with the path API.This means my app just cant access any path

*tauri file watchers are saf and secure

*Use your own Rust binaries or custom Rust code for low-level or app-specific file system operations where you need full control or specialized behavior.
*Use Tauri plugins like the file watcher plugin for complex, cross-platform features that are already well implemented and maintained.
*This balances control, security, maintainability, and development speed, playing to the strengths of Tauriâ€™s architecture and ecosystem.

i had to change permissions and capabilities

*so using variable is type. is used for creating type guard functions

stale reads
race conditions

runtime safe doesnt mean its logic prone.

false type errors

*most logic errors arent complex but just overlooked small mistakes that are good at hiding themselves in large codebases making the developer think an error occured as a result of a higher level logic.This is because they dont crash at runtime and are type safe,so its often believed that it worked as expected so one can think its something he forgot to do somewhere.they can be based on assumptions,incorrect condition checks,etc.assumptions can still be made even with type safety because a type doesnt describe the particular context of the data.I think the best way to avoid assumptions is to log data that you arent sure of but the other logic errors need good attention to detail

*testing can prevent logic error

*a way to ensure predictable behaviour is just to return the modified array to the caller so that regardless of mutation,the caller gets a consistent array state.If you mutate an array inside a function but donâ€™t return it, the caller may not realize the array has changed, especially if the reference was reassigned internally.



smart loading cache validation is false on startup incase any changes happened to the fs when the app was closed as the app cant track it

the aot caching state is directly tied to the opacity od the app
the loading message is directly tied to the opacity of the files

added auto refreshing on top of smart loading

todo:dont make the opacity of the ui tied to string data

Breaking down a large task into smaller functions can give you a sense of progress and control. It feels less daunting to work on a series of small, manageable functions than to tackle a single, massive block of code.while the total number of lines of code might not decrease significantly, organizing code into smaller functions reduces cognitive load, improves readability, promotes reusability, and creates a psychological effect that makes the code feel smaller and more manageable.

lazy dir reading
lazy file reading
caching opened dirs
cache mamagement
selective ahead of time caching
cache persistence
Incremental ui loading
selective smart reloading
auto reloading
background auto reloading
on demand recursive file search
quick search

time complexity optimizations
implementation optimization

multi threading read dir

*wrapping try catch in several areas of the codebase unnecessarily will only catch symptoms not errors.It wont help you find the root cause of a problem

include,startswith,endswith,equality,particular case conversion for case insensitive searches
glob patterns
regexp
search library--typo tolerant,name based search but not content based search
search engine--content based search
vector database--content based search

*Types of search
keyword search
fuzzy search
index search
semantic search

low threshold--strict check,less noise
higher threshold--loose checks,more noise


prolog
natural language processing
vector database in rust

if search result is none,display the fsnode array

ill use debouncing on the search

*The tooling ecosystem is beautiful

for an array,i use three states.null means its absent,[] means its present and [value] means it has values

*from now on goin forward,ill do direct toasting in my thunks but gradually since a number of them still use the component.My toast component has lots of issues

sidebar,body,topbar,fsnodes

*so animation costs performance and as such,it must be selective.They must only load at key moments and not continuously

^debouncing is better for actions that depend on inactivity like use input
^throttling is better for continuous actions like writing to a disk

*debouncing with leading execution is seamless which searching.its what ive been looking for.debouncing with trailing execution makes it feel a bit laggy but when it comes to writing data to a db,throttling with trailing is better than leading because it ensures that the writes are uo to date

*so both throttling and debouncing optimize call frequencies but debouncing main concern is the final or initial state of data without caring about the guarantee that it will execute  while throttling main concern is to regulate function calls but also has caring about the initial or final data as a secondary concern.initial debouncing prioritizes responsiveness while trailing debouncing prioritizes the final data by leveraging inactivity

strict and loose option

*so stale state reads in react since states updates async can be avoided by passing it as a parameter to the function that relies on it or putting that function in a use effect 

*so debouncing is meant to have a smaller time window while throttling can have a moderate sized time window depending on use case.Why?Debounceâ€™s goal is to reduce noisy rapid calls by waiting for a pause, so a smaller delay keeps the UI responsive without flooding calls.Throttleâ€™s goal is to limit call frequency over time, so the interval can be adjusted based on how often you want updates or saves to happen.

*so because the dir is large and there will be more files that may be relevant to the search,a threshold of 0.4 will be better and ill increase the debounce timer to 500-1000ms.

*Directory size and variability: As you noted, directories can be very large or very small, and itâ€™s hard to predict which directories will be searched again during the process lifecycle. Caching the entire recursive directory contents would consume significant memory and may quickly become stale or invalid.

*Caching entire recursive directory reads is often impractical due to size and unpredictability, but caching search results keyed by query and path is a practical, memory-efficient way to improve search performance and user experience.


^recursive search requires trailing debouncing unlike surface search that is better with leading debouncing

^never await a throttle or debouncer

*useref,usememo,usecallback

*i removed debouncing for the search.the search only proceeds if the user presses enter as they are no async solutions to debouncing


todo:spread search results.it will affect that output of no search results.ill leave set search results and use it to clear the search if the matched fs nodes is empty

what happened to the cache
i have to increase the batch number to 20
^recursive search will become a problem during stack overflow

freeze the ui when searching
terminate search,exit search results
lazy dir reading + file search + setting search results
recursive dir reading + file search + setting search results
recursive dir reading <-> filesearch + spread search results
batched searching--to prevent ui flickering and reduce fuseinstances
only spreading when there is a result--to prevent ui flickering
memory clean recursion
batched search but only updates the fuse instance not recreate it
quick search--applies an aggressive filter on the data before searching
search batch optimization:The batch is small at first for responsiveness and then bigger later
quick search--does early termination and fast batch skipping on long queries
quick search doesnt sort results,full search does
quick search doesnt show progress
make the quick search a nice button
pre sorting based on mode to increase the chance of seeing your result early
breadth first recursion over depth first
useTransition optimizations
progress is at certain points because of rerenders but the styling updates flawlessly
switched to early best-first traversal
todo:fix the rendering of the arrow when in the home folder

i need to sort the array by files first before folder

^The state management paradigms is the one of the major studies in react

*for some reason,my recusrive search can;
not read:downloads,documents,home and throws an error
read:recent,downloads,pictures,music,videos

*The reason why recursive search failed want because of any file system or permission issues but because of the way the function was written

todo:the read dir function throws some errors when used in the recursive search function but its safe to ignore for now.its correct i believe its with fs config or something

*so by default,search libraries and engines do a full search but quick search works by aggressively filtering out the data using cheap methods before giving to the search library for analysis.its cheap because its easy to filter out items by using rules

*two things.so quick search/aggressive filtering reduces the objects for processing down to 70-90% which is good but secondly,i had to embed the search with the recursion because i tried doing the recursion first then search but it had lack of transparency about the progress of the app.the app remains stale for minutes without seeing any result till later.so i embedded the search directly in the recursion.as it recurses through a file,it searches it for a match but because of ui and performance,i batched it to 20 to be searched through at a time per recursion.if the new fuse instance is a bottleneck,i can easily move it to the app state so that only a single instance will exist

*so bigger batch size will increase performance by reducing the number of fuse instances per update but it will reduce ui repsonsiveness.But to make this better,i can combine it with set collection which creates a double optimization.It only updates the collection only on every 20 updates or so

*The quick serch + the batch + updating single fuse instance increased the speed by a margin.If i didnt embed the search to the recursion.this wont be possible


*Serializable values are plain JavaScript data that can be safely converted to a storable format (like JSON), such as:primitives,plain objects and arrays while functions,class instances,promises and anything that cant be serialized with json.stringify are non serializable values.

*so redux performs serialization checks on thunks and if a non-serializable value is within a thunk,it will throw an error and fail.so if i want to use unserializable valye,i have to handle dispatch and get state manually.If you manually handle dispatch and getState inside a regular async function (not as a thunk), you avoid this middleware check because youâ€™re not dispatching non-serializable data as actions.Redux state is menat to be kept free of unserializable data because of debugging and as such,escaping it like this isnt recommended.

*so to prevent unserializable data problems in redux that can cause debugging later,i should move it to janother state management solution paradigm that allows this without compromising any quality like the atomic state management of jotai.Newsflash:I dont need jotai for that,i can just move it above the app state in the same file and use it globally across thunks creating a global singleton.This means that singletons which are very uncommon have the use case of performance where recreating an object is costly

*so unserializable data in redux can be solved by moving it globally above the app state and singletons have discouraged use cases because of shared mutation but redux uses a singleton object to store the app state.its safe here because reducers which causes state updates are immutable by defaul.They return a new state without modifying the old onet.The syntax of mutability under reducers is actually using the immer library under the hood which makes immutability look like mutation.Im also using a singleton fuse instance but its safe since im only mutating it in one place under a single threaded environment and its a utility object (index) rather than application state.So singleton are best when constructing a new instance of the object will be too expensive and it makes sense for it to be used globally.but i belive this is the only encouraged use case of a singleton.

so you are telling me that i can create a thunk that returns an async callback in a normal thunk.so i await outside the dispatch like await dispatch thunk.what is now a thunk that has a type annotation of Promise<AppThunk> that returns an async callback and i call await inside the dispacth like dispatch await thunk.whats the difference.The promise thunk is a redundant pattern.refactor to the simpler one.This is becuase redux thunks can be typed in various styles.

*so there are three async thunk patterns

*Refactoring large existing codebases all at once can be risky and time-consuming. Itâ€™s often better to stabilize the project first.You can keep the current verbose thunk patterns working and stable for now.For new features or ongoing work, adopt simpler, idiomatic async patterns (like createAsyncThunk or properly typed async thunks that return promises).Over time, you can gradually refactor older code to the new pattern as you touch it, improving maintainability incrementally.This incremental approach reduces risk and spreads out the effort.

useMemo,useCallback,React.memo,useRef,useState

*more layers of dispatching makes the app slower but it can encapsulate logic for readability

native ui toolkit--provided by the os
web renderer--provided by the browser--has wide support for 3d and animations
flutter engine--pixel perfect,has support for animations

*,im attempting to speed performance by only searching when 20 files have been stacked.This increases perf by reducing the number of times i call fuse search which causes overhead but results are seen less quick

*stale data theory encourages that variables should be scoped to prevent unexpected data races.If it were to be global because of persistence,it must only be used and modified in one place,if it does not need to trigger UI updates or be tracked as part of your Redux app state and you want to avoid unnecessary complexity of storing transient control values in Redux.Meaning that global variables in redux are best for configuration that dont cause the ui to react.This is because redux state is expected to trigger ui updates

^batch processing vs stream processing

*so the working principle is that some optimizations in general can be present but depending on the input size or the device,the effect may be subtle to significantThese types of optimizations are subtle optimizations.They are meant to improve the worst case scenario of performance to  something better while other optimizations provide significant result regardless of the device or input data.These are algorithm optimizations.

*because shorter strings are naturally fuzzy,early termination wont be useful but longer strings usually demand exact matches so early termination will be useful.Early termination can also be useful for shorter queries if the queries are semantically complete like add.svg.

sorting based on score only if quick search is off


*Why early termination on first exact match may not always work
*Exact matches might appear anywhere in the dataset, not necessarily near the start.
*If you stop searching immediately on the first exact match, you might miss other exact matches deeper in the tree that could be equally or more relevant.
*If the dataset is large and the exact match is near the end, early termination wonâ€™t help much because you still have to scan most of the data.
*Sometimes, no exact matches exist at all, so you still need to run the full fuzzy search.

*if i do sort,it may take time depending on the sorting algorithm js uses and how long the tree is.it may outweigh the benefit of early termination.so they are two solutions.first,i can choose to make this feature happen only if quick search is on as it indicates that the users expect to find results quickly and secondly,i can check for exact matches for every batch instead of ever node to increase the possiblity but not guaranteed to capture more nodes that also have exact matches down the tree and since quick search cuts down 70-90% of nodes using cheap filtering,the possibility that exact matches will be included in a bach is also higher.so if i do that early termination check here,i can capture a decent amount of exact matches not to mention the user doesnt expect every match to show since he said quick search

*searches is meant to be case insensitive

*it sets the search result once it finds an immediate match
check for exct match on file extension if present in the node
how did many of them even survive that dangerous filter
the exact match is insensitive to file extensions in case of folders

*so i should always have it in the back of my mind that my app may not behave as expected under edge cases

*i think a better check is to remove the dots from both the fsnode name and the search query and check if the search query is included in the fsnode name.in case of false positives,what if instead of including afterwards,it does startswith instead.so car wont match for scar but it will for cargo.it reduces the false positives.so if it doesnt match exactly,then includes can be a fallback but a query thats 10 + chars long probably knows what is saying so the startswith already reduces the probability of returning a false positive and having includes as a fallback is as good as not having starts with which increases false positives.and since this is done per batch and not per node to catch a wide array of false positives,i think it will be better to accumulate the results of existing matches instead of returning on the first match so that if doesnt return a false positive but a series of possible results.since all of this happens when the search is quick mode,i believe that users dont expect to get definite matches so having an accumulation of matches not just a single exact match is acceptable.The thing is that it will run the entire batch but its only 15.its less expensive than running search util.If it doesnt match at all,it proceed to fuzzy search as a fallback.this optimization only applies to long queries.for short ones,the default fuzzy search will work


*the reason why it changed from remove ext to removing all the dots + prefix matching is because of edge cases and i believe it works well for a variety of them at the cost of more search results but with the intention that the search is on quick when this happens and the fact that quick filters out many of the nodes and its done per batch means that it wont just terminate the search on the first match but possible matches.The filter happens as it searches recursively but it doesnt need to be ahead of time filtering because only filtered nodes escape to the batch and also,trying to use a robust extension parser is error prone and can be complex

*i reduced the query size required for this to something small for testing and it works flawlessly.cargo matched fifteen folders at first but when i reduced the query length requiremnt to 5 to check if early termination worked,it reduced the search to just 5 and it happened that the folder i was looking for was there.The probability is high but not guaranteed

*its true that the chances are high and not guaranteed but the exact percentage depends on the distribution of the fs in my app state,filtering logic and batch size. Larger batches increase the likelihood that your target node appears in a batch, but also increase processing cost.this is expected of quick searches.they are meant to be approximate results.so when will users need quick search over the more accurate deep search that doesnt filter or terminate early

*so quick search even if it may not feel all that quick to the user from the deep search actually performs optimizations under the hood.the performance will be visible when the folder is very large or if the device is slower and if the device is already fast,having it faster wont hurt.

*the third and last thing i want to add to quick search thats not in full search is that full search will sort the results based on score before returning wile quick search wont.this will make quick search seem fatser as full search will have to sort the results based on relevancy.is this needed

*since search util oly gets calledper batch,it doesnt return large results but it returns sorted results only for that batch.so sorted batches will be added infront of other sorted batches that may have less relevancy so i believe this should happen at a higher level than the search util:It can happen at the very first function that initiates the search 

remove the dots
do a starts with
accumulate result

a better way to force quit the function

*i cleared the scores immediately afer the paired result because the ui doesnt react to it so i was attempting to free memory early but despite the micro optimzation,i should dispatch it after for clarity and readability that im not using the state only after sorting.It minimizes the risk of subtle bugs if any UI or middleware unexpectedly depends on scores during or after sorting.The performance difference is typically negligible unless youâ€™re working with extremely large datasets or very tight memory constraints.

changing app state vs routing

*set timeout in js is used for implementing delays,throttling and debouncing.debouncing is mostly used for search box and infinite scroll

*one thing i changed in my update search result was that before,as you know,if a query is long and quick search is on,it will attempt to check for very rough matches without involving fuse and if theres a rough match,it will terminate the search and the results will be displayed.the thing is that you insisted that it should fallback to fuse through the searchutil function if there were no rough matches but if so,it will even be slower than full search if the query wasnt present at that batch.for all we know,the result that the user is looking for may be in another batch and not the current one so wasting resources by falling back to fuse if none was found isnt the best approach i believe.so what i changed it to is that if there werent any rough matches in the batch in quick search mode,it doesnt fallback to search util but rather,it clears the batch and returns so that another batch can be processed.This optimization for quick search will discard a lot of results so using a query length guard when the query is long can ensure that the query is intentional.This ensures prefix matching only activates for longer, more specific queries where prefix matches are likely intentional.

*many modern file managers use indexing for file search and fallback to a full fs search when the search isnt satisfied by the index so they dont provide options for quick or full search.but it has the complexity of being stale.mine uses lazy recursion searching where the search lazily recurses through the directory till it finds the targeted file.depending on the mode,the recursion can be very fast or full but the recursion in general is optimized enough.As it finds matched queries during the recursion,it updates the ui.it doesnt recurse everything first before performing a search.otherwise,there will be a lack of progress and quick search wont be possible because quick search operates on the fly.

indexing
recursive search
lazy recursive search,quick search or indexing support

^Perf on search checks
*the difference between quick search and full search on a small folder when full search performed a sort on relevance was 3.123 for quick search at its best and 3.338 for full search at its best.the difference here lies more in the sort than the cheap filter.but without full search performing  a sort,its perf at best was 3.365 while quick search was 3.113 because of the cheap filtering.so quick search outperforms full search with or without sorting but full search normally should sort because its expected so quick search outperforms full search at the cost of inaccuracy

*For small folders, the difference is small but measurable; quick search still outperforms full search.For large datasets, the performance gap will likely widen, making quick search even more valuable for fast feedback.Users expect full search results to be sorted by relevance, so skipping sorting in full search is usually not acceptable so the perf based on sorting is more accurate.so based on this and on a small folder i searched on with a particular query,quick search at best is 3.123 secs while full search at best is 3.338 seconds.when i performed quick search on a long query that is >= 10 chars longs,quick search finished at 1.858 secs at best while full search finished in 2.705 seconds at best while quick search finished at 1.937 secs at worst while full search performed at 2.872 secs at worst

*i believe my quick search is best when you know part of or the entire spelling of a file or folder regardless of case and dot placement because it has a high chance of escaping the aggressive filtering and batch skipping of quick search while full search is when you don't really remember the spelling correctly or if you want similar results to show

*the only thing that can spoil my recursive search is stack overflow

i can make my search faster by swapping out the search library i used with a faster one since the use of the search librray is just in one small function.my search logic itself isnt small.its made up of four robust functions as i showed earlier but the function that actually uses the fuse instance is small.thats where i can easily change the library without affecting the search logic of my app at all.if the fuse js instance were used all over all the functions that constitute the search,it will then be a problem because the search library is directly baked into my apps functionality

so if i want for example,i can switch it with a rust version for optimal speed but this is only a note not a necessity.I will just compile it to wasm and use it instead of a tauri ipc protocol

web worker to make it faster

use rust search instead

*truncating a string to an ellipsis (...)

*the fragment element <></> is used for grouping tags or components in jsx when you dont want the grouping to affect their layout as it is for div and other semantic containers.i use it especially when putting conditional rendring in my code as it has to be under a tag.

*the data you are mutating in a recursion should be outside of the recursion to prevent it from resetting

*so the problem with my function is just the fundamental limit of computing.no computation can consistently or predictable modify a shared state like an array over a recursion.its just isnt possible

*arrays can be used across recursions oly when treated as a whole like in batch processing but when indexing particular values,records are better

*so if a folder has 10 folders,i can show overall progress for each folder that completes but it will be in 10 units because the search doesnt know the number of files under those dir until it recurses through them so their own individual progress will be under their record

*the reason for this bug isnt what you expect.it failed this entire time because as quick search was on,90% of the nodes werent processed so they werent updated

*so what i did to fix this bug is called dismantling where i had to painfully teardown a function big chunk of code into simplest form so that i can understand how the bug works.i now have to use version control to add back the complexity little by little while ensuring its correctness instead of reverting back to the complexity and trying to add the fix i discovered over there

*Yes, what you did to fix the bug is indeed a well-recognized and effective debugging strategy often referred to as dismantling, simplification, or reduction. It involves breaking down a complex function or codebase into its simplest working form to isolate and understand the root cause of a bug. Then, using version control, you incrementally add back complexity, verifying correctness at each step.

*However, reverse engineering also applies when you analyze any system or codebase to gain deeper understanding, regardless of whether you originally wrote it or have source access ().

*You took your own complex function and dismantled it into simpler parts to understand and isolate the bug.

This process is essentially "going backward" through your codeâ€™s logic and structure to recover a clearer understanding - which aligns with the concept of reverse engineering ().

Even though you have the source, the complexity or lack of documentation makes the code effectively a "black box" that you need to analyze and comprehend.

Your use of version control to incrementally rebuild complexity after isolating the issue is a practical application of design recovery and program comprehension, subfields of reverse engineering ().


*before,every recursed file shared the same batch,now each recursion call has its own independent batch

passing redux state across function calls will save perf instead of reselcting
passing an object is unnecessary and can cuase perf if you only need one part of it in the calling function.pass that part instead.


arbritary order
alphabetical order
modified date order

*the cost of pre sorting this is a bit of performance but rust sorting should make this time negligible even on larger data sets.its worth seeing the result earlier in the search and if they do,the users can terminate the search from searching further.the time it would have taken to search further may even be significantly larger than sorting not to mention the sorting is in rust.so this is an optimization on both quick and full search while other actions like opening the dir can be in arbritary order

*i think ill leave the score sorting in js for now until i see a real perf bottleneck.cuz the rust part is a bit ocomplex to read and mange the genric

*i did a benchmark and sorting the array based on search mode before processing has little perf impact.i got 3.1 with the sorting and 3 without it.so the speed is nearly the same or if not the same in practice with the benefit that you may see your results earlier rather than later down the search

*the search is already as optimized as it can get.batching,pre sorting,lazy recursion with ui updates along the way,the perf boost that quick search has:applies an aggressive filter on the data before searching,does early termination and fast batch skipping on long queries,doesnt sort results,it doesnt show progress and the batch changes from 5 for responsiveness to 15 for efficiency.so thats all.The search exp is complete

*the diff for another query i did is 2 seconds even for the full search.so depending on how large the folder is.users can use my full search for insights even if its just a few seconds extra while the default is instant but my quick search option is good for instant results even if its just two seconds late and sometimes,the default file manager can take its time and the only note of progress is the results that pops up and a progress bar that goes to 99% and stops for a while.mine shows progress bars on folders as they are being recursed whie also showing results in full search mode.so my file manager can stand out in search experience

showing progress


^Backup search optimizations
a rust search library
selective web workers for recursive search.cam be applied to the heaviest part of the search
score sorting implemented in rust
indexing
caching terminated searches only so that if searched again,it can continue where it left off


^UI OPTIMIZATIONS
Redux web workers
Memoization
Throttled reducers
Breaking down components
Virtualization
useTransition
million js
react window
decoupling the redux state to a local state in a react component that is under a transition


*so if i want to tranistion a redux state in a component,i should create a separate state that is of the same type and under a useeffect,whenever the redux state changes,i update the state under a transition.ill use this to render instead of the redux state directly

*so my search could have been a second or instantly for all we know if not for the ui.so to optimize,i have to throttle ui updates,memoize components,restructure my components for isolated re-renders,offload ui updates to web workers or most effectively,virtualization,useTransition

virtualization with react window and throttling

*so these are my backup optimizations for the search only in case of bottlenecks and if i do implement them,it wont be all at once but only the one best for the situation

*so removing the await will work but will become unpredictable because it runs concurrently but by seprating the loop into two separate ones,it becomes safer and more predictable.I also believe breadth first traversal will provide results quicker because it wont waste time just in one folder before searching the next.so breadth first and depth first traversal have the same time complexity but breadth first evenly distributes progress which can give a sense of speed and concurrency because it makes more progress in a smaller amount of time even if the overall time is the same.so because DFS "remembers" only the current path it is exploring meaning its space complexity depends on the maximum depth of the search tree and BFS "remembers" all nodes at the current frontier level meaning that its space complexity depends on the maximum width of the graph at any level (O(w)), which can be much larger than the depth in wide graphs..BFS consumes more space as a tradeoff for distributing progress.bfs is best for wide graphs while dfs is best for narrow graphs.i believe the fs is wide graph so bfs searching will be better for showing progress.

*simple code structure changes while still preserving its functionality can change progress distribution and space complexity but the time complexity remains the same since i ddidnt deduct or add anything

*so breadth first traversal and presorting are progress based optimizations not time based optimizations

*with the breadth first traversal,i got many results instantly but it still finsihed searching at 3 seconds.while back in the depth first method,it sill finished at 3 seconds but i only got 1-2 results before the rest later in the 3 seconds.this bfs alone nails my searching exp after i taught it couldnt be any more optimized

ui optimization
progress optmization
subtle optimization
algortithm optimization--time and memoy optimizations
code optimization

*i just realized that ui affects the speed of the search cuz if the search updates the ui,the ui has to repsond to that change before the search proceeds.because i added more ui elements for showing progress,it directly affected the search time by a second

computation vs rendering
eagerly computed but lazily rendered

*i think for my use case,using use transition without throttling is better because depending on the folder thats being searched in,results may not come too often and delaying results when they do come when they are very little will damage ux.with use transition,i get 0.1 -0.2 seconds difference than without it on a small folder with a short query.so as the folder gets larger,i believe the benefit will increase

*so because in a folder many batches are done rapidly in a sync manner,react never gets a chance to update per batch but since folder processing have awaits inserted in between them,i give react a chance to update meaning that computationally,progress is happening per batch but react only renders per folder in this scenario.

todo:transition the search progress

*so it will optimize perf,not show inc progress but what if i have two forms of updateprogress one async and one sync.if im in a certain percentage of the batch,i use await update but for the rest,i use update so react can update progress displays across certain points in the batch but not at every point by always using await because it will degrade perfa

*so by using await at certain thresholds of processing and not at every point,i dont force rerenders and i can increase ux by showing incremental progress.ill  then use normal updates for the rest.so this takes advantage of when react updates while also understanding that too many updates will destroy perf since this is done recursively.so theoretically or ideally,my progress bar will render per batch but under real scenarios of perf,this is the best approach

*another thing is that i wont want the progress bar to be jumping,it wont look smooth to the user so render-wise,its at ceratin points but styling-wise,i can apply transitions at the style level so that when it jumps,the progress bar visually moves to that point instead of jumping directly.so users will think its continuously inc not knowing its just styling

*so computationally,progress is continuously updated,render-wise,its at discrete points but styling-wise is continuous

so using exact equality on a floating value is fragile.using a threshold like >= is better.using set timeout will queue it as a macro task which will be slower than queuing it as a microtask because it will be uneccessarily deferred so using promise.resolve allows me to resolve on a promise which isnt there but it allows my update to be queued as a mcrotask.so from an effect standpoint,they both defer it but paying attention to the techniques used shows that they are different

the more hallmarks,the more progressive the bars but the slower the app.balance is needed.every benefit of a coding strategy comes with tradeoffs.i think the 5 thresholds will do.css transitions can make these point transitions flawless

*so this means that redux reducers should be pure and indempotent relative to the app state

*so the problem is that using strict equality is fragile because of floating point,i cant use >= which means when crossing an upper bound because it will return true for all intermediate values above the first threshold so i think the best approach is a range.so it wont be discrete thresholds like 20,50,100 but ranged thresholds like 0-20,40-60,80-100,.if i use ranges,they must be padded to ensure that it doesnt become <=100 scenario which will always return true.

*You use .some() instead of .forEach() when you want to stop iterating early as soon as a condition is met, because:.some() stops and returns true immediately when its callback returns a truthy value..forEach() always iterates over all elements and cannot be stopped or broken out of early (except by throwing an exception).This means .some() is more efficient when you only need to check if any element satisfies a condition and want to avoid unnecessary iterations.so it only requires the last threshold not because it will unnecessarily update the search progress twice for the same path making the data be false and inconsistent

*so its because im using range,im using last threhold tracking.in an ideal scenario,thresholds should be at strict equality but in practice ,this is the best approach unless you want to round it but multiple numbers can round to the same value and can cause duplicates but last threshold tracking can prevent that here so the true reason why we dont go for the easy way to reach the ideal solution through rounding because values that are very close to the threshold may round off it even a little missing the threshold like 50.5 will round to 51 but there is no threshiold for 51.so the reason why my code failed before was because i was thinking of it working under ideal conditions

*so code that looks logically sound will misbehave just because they were made under the assumption of ideal conditions.so there are ideal conditions,practical conditions and edge cases.making your code under the mind of ideal conditions may or may not cause it to break.its a small chance based on the circumstances but making your code with practical conditions in mind ensures that your code behaves correctly in a wide array of cirucmstances.only that small 2-5% are edge cases

strict equality
threshold crossing
ranging
milestone progress

*for small folders, itâ€™s normal and expected that you go straight to 100%. There simply arenâ€™t enough nodes to hit intermediate thresholds.

*In languages like Rust and Python, where asynchronous concurrency is not the default or native model, stale updates and subtle bugs related to concurrency typically arise only when you explicitly use threading or async features.


?adding to searched nodes is still stale but since i have the lastThreshold,i can just use that as the percentaege.

generating ids on memo isnt stable

*due to Reactâ€™s concurrency and batching behavior, especially with tight synchronous loops, itâ€™s very challenging to track and render real-time progress for individual folders being searched. React batches state updates and defers rendering until the event loop yields, so UI updates inside tight loops often appear only after the entire loop finishes.

electron --node js services
tauri--rust services

optional chaining,undefined and type assertion X
null union is safer,null coaellescing operatir
result union is safer for error handling


broswer services,
node js services
rust services
web server--can only access the browser
application server--can access node js services

tauri window--uses webview,can only access browser services
electron--uses node js instance,chromium and as such,it can access browser and node js services in the same project

a js file can only run either in a browser context(client) or node js context(server) but not at the same time meaning that a project that requires the two will require two sep js files for each of the context

react app-full client side and can only access browser services
nest js service---full server side,can only access node js services
electron--client side apps but with access to node js services
tauri--client side app but can access rust services
next js--server side apps but can use client components to access browser services

there is backend services and system services

backend services are separate from project dependencies.the diff is that dep are installed alon side the project while services are consumed by the project from another sep codebase through an api.

system services are provided by std lib like node js or rust libraries

technology,framework,solution,a service,an addon

a dependency can be either client side only,server side only or context insensitive. i.e it works regardless of the env because it doesnt use any service special to either side to work

std lib is a system service,
database is an external service

js lacks a std lib so node js is the only available ste of services for js

hooks transform data
memoization

context api
hoc
global state solution
state paradigms

code optimization
algorithm opt--time,mem

checking against ids vs boolean array for selecting ui elements
swapping out content vs routing
svelte compiles to normal dom,react compiles to v-dom
svelte doesnt use keys,react does
styled components
sfc,function,classes
react keys--everything within a map that can render dynamically must have a key
component lib--styled or unstyled components
apine js--quick websites
astro js with react
hugo

*website
html,css solution,js
hugo,css solution,alpine
astro,css solution,react

*desktop apps
tauri
electron

*server side apps
next js

astro--mdx,partial hydration,ssg,seo,speed

markdown in components vs mdx.they are functionally the same but mdx is easier and doesnt require breaking your cotent semantically to fit in your components

ssg framework
baas
full backend api

in memory cache,disk cache,in memory cache serialized to a disk

service workers and web workers
state solution,pouch db
server state solution,database

online web app--server side state logiv,a minimal client side state solution like jotai for caching and a backend as a service

desktop app--redux state logic,rtk query to communicate with a baas or a full backend.

The key is to avoid division of logic across solutions as solutions are supposed to be self contained

networking
system level interactions

react and redux--self contained client side app solution
next js--self contained server side app solution


external servers are now used for building services and no longer full stack user apps.They were used to make mpa apps using a templating engine like pug or ejs.Examples of backend frameworks;graphql,django,ruby on rails,nest js,phoenix,trpc,socket io,spring boot,node js,express js.They are used to make services like;
complex algorithms--machine learning models and  large scale entreprise algorithms
legacy bridge
specialized services--socket io for real time communication
security services--for authentication and secure server side operations like payments
supporting services--graphql service for querying data

docker is used for distributing backend servers
docker or platform as a service like vercel are used for distributing online web apps and can be connected with url on the browser or a client side window like a tauri desktop window which is just a view into a web from a url 
installers are for distributing desktop apps using websites

a web app can be desktop client app,browser client app,broswer server app.

react + system level--tauri apps
react + desktop app exp---electron apps
react + networking--next js apps
react + mobile--react native apps
react + website--react and astro websites

*because state updates in react happen concurrently,its unreliable to print the value of that state before or after state modifications.it has to be under a useeffect

*spreading blindly is a goof way to preserve uniquely generated ids but it wont work as intended in every situation if there isnt a mechanism to invalidate the previous state and intehrating that into thunks to communicate that to the component is difficult.that technique worked in some cases like search results where the results is only used in one place but for something like a folder that is opened or used in multiple functions,it becomes difficult.the best apprach here is to extend the map with logic to reuse prev ids

*using ids and memoizing on that id
*spreading or extending the map to preserve ids

*the reason why it felt laggy after i timed out the second dispath wasnt because my component wasnt listening properly but because the caller didnt wait for the second to finish so it ran all the other code it was supposed to do after loading which delayed the ui.

*so in general,settimeout shouldnt be used to instruct the react runtime to flush ui updates but rather through promises and awaiting them as they are more predictable

*the reason why im doing this instead of letting react to batch the ui updates is that fsnodes can be a big array and rendering everything at once can feel slower and less responsive so by rendering only a small subset of  it,the switch can feel instant and react can render everything else afterwards.

static key--direct property in jsx
composite static key
dynamic key--uuid as a derived state or directly
stable dynamic key
unsafe dynamic keys--indexes

stable dynamic key
------------------
locally stable--->useEffect,setter for prev value,map/spread
globally stable-->directly on the app state and used in the jsx

tracking,memoization

locally stable dynamic keys--useEffect with a setter and a map or spread
globally stable dynamic keys--use redux state

*i dont have to worry about the order or anything like that.a node in the fs has a unique path naturally so if the paths dont match,a new uuid is generated.this also means i should have made the keys the path instead of unique ids but its what it is

*so there are two types of keys;static keys,dynamic keys and stable dynamic keys.for a key to be preserved,it must be static or stable without falling short of being unique.a key is static if its naturally part of the data as under the app state like the path to a node in the fs.it doesnt come from the algorithm but if it doesnt have any natural uniquness,it can use dynamic keys which comes from the algorithm.but dynamic keys are regenerated on every component rerender which is good if the key is just used for tracking but if its needed for other things like memoization,then it must be preserved.a key is stable and dynamic if there is some way of preventing the key from being regenerated on every render.its either the dynamic key is generated ahead of time under the redux app state because the app state remains stable ad global as its isolated from component rerenders or the key must be set only once the data changes under a use effect in a component.This is a local stable dynamic key as opposed to a global stable dynamic key as stored in the redux state.Its stable because it doesnt change on every rerender but only when the data it requires changes but it doesnt preserve the data until spreading or mapping is done.spreading is simple but blind as it cant apply to every context while using a custom map function can include logic on where to preserve keys according to context.mine now uses a static key.it doesnt change whenever i load my program,its dictated by the path on the fs because they are naturally unique but before,i was trying to use local stable dynamic keys but it wasnt required for the kind of data i was handling.indexes are unstable dynamic keys cuz as opposed to regular dynamic keys that remain unique even though regenerated on rerender.they cant be used for tracking nor memoizing.its just the easy way out of react complaining about keys.its only for lists that their elements never change dynamically.i also realized something.i cant use state hook on a dynamic or static key cuz it only changes when the component rerenders but not necessarily when data changes.thats why when i used useState for my static keys,it didnt work as expected so i had to use useMemo and attactch a dependency so that whenever that datat changes even though the component doesnt rerender,the variable is recreated with the latest version of the data equipped with their static or dynamic keys.its when im trying to create locally stable dynamic keys,that i use a setter under a use effect with either spreading or a map.this is because useState alone is not reactive to data changes unless you update it explicitly.i think index keys are better termed unsafe dynamic keys. than unstable. a key is stable when it doesnt recompute its values on every rerender by preserving old key values i believe.so if a key is purely for tracking and not identity,then a dynamic key will work but if its used for identity for memorization,then using stable dynamic or static keys is better.i know that svelte doesnt use keys because its compiled so there is no runtime required to track nodes.

*i just realized that the useMemo for  dynamic keys is a way of creating a derived state which is a state that is create from reacting to another state.i also rrealized that for static keys,i dont have to use a derived state that maps through the data and returns a structure that has an id with it,i can just directly use the unique property of the node in the jsx.this saves perf by deducting the O(n) complexity of mapping and producing unique keys.its for dynamic keys that are generated on every render,that i use a derived state.so real world data has uniqueness of some sort but use generated content may not have uniqueness cuz in a kanban app,i had to generate keys because task cards didnt have any uniqueness.these are the two types of stable dynamic keys.

*the first one is globallly stable cuz its generated upon creation while the second one is locally stable cuz its generated upon rendering.the first one is easier but the second one requires a map or spread under a setter to preserve ids.it cant be derived state because it wont be preserved.locally stable keys are used when you dont have direct access to the model but for a redux/react app,you definitely have access to the app state model but i believe some may still use locally stable keys when they dont want to touch the app model as it wasnt built with setting keys in mind.regular dynamic keys can be used directly in the key prop of a jsx but the problem between this and using a derived state is that it can unnecessarily rerender when other component data changes.so derived state is best for regular dynamic keys.i just realized that useEffect + SetState = useMemo derived state but for locally stable keys,the reason why we still use the setter under the useEffect is that the setter gives access to the previous value allowing preservation to happen.useMemo creates an optimized derived state but it doesnt preserve values.

*so for user generated data that dont have unique keys,i can create a composite key by combining context information with timestamp data while real world data that naturally comes with uniqueness have a single unique key not a composite one.using a composite key for content that doesnt have uniqueness provides static keys that can improve rendering performance over using an id algorithm that requires making a stable dynamic key.so stable dynamic keys should be preferred over composite static keys when the context of the user generated content can switch easily between contexts and when timestamps may lack precision but under normal scenarios though,users dont create task in the same millisecond or even the same second or 5.so stable dynamic keys are more robust and well fitted for edge cases than composite static keys.composite static keys provide performance benefits under typical scenarios.so user generated content for a chat app can reliable use composite static keys but that in kanban boards where task cards can change rapidly between groups requires stable dynamic keys.so a composite key is a way to provide a static key to data that isnt inherently unique like user generated content as opposed to real world content like database records or fs paths which have uniquness.

identity operations:selection,tracking and memoiszation.for the selection,you can either use index or a key.index is unsafe.for a single selection,you can use a state variable to hold the index or key of the element currently being selected and performa n equality check to know if the element was selected or not.for group selection,you can use a flag or an array of the selected elements.a flag for index is a boolean array and for keys,its a record but an array of the selected value keys or indexes is better for performance

useState,useEffect,useMemo,useTransition,useCallback,useRef,props,selectors
memo

^paradigms
reactivity paradigm
rendering paradigm
state paradigm
key paradigm


vscode,node js,git,postman,mongo db,postgres
express js,nextjs/react

code editor and rest client or browser

*i used an array for both my in app state cache and fsnodes.the fsnodes[] array is used by my components directly for rendering so it makes sense to leave it as an array but cached data is also an array but its only used for lookup.this made me to loop through it to find the relevant cahed data im looking for costing performance.the same for this folder index.data structures that are used for lookups should be records not arrays for perf and ones that are directly rendered are better off as an array for easy rendering in a list


heuristic skipping
using index data to skip folders
using index data to prioritize folder search based on their media files

blind recursive search
intelligent recursive search

record over an array for 01

array is great for rendering but record is best for lookups

custom types can be an interface or a type alias

*Recursive searching has O(n) time complexity
*Your traversal is breadth-first only at the current directory level, but depth-first for all nested subfolders.
*bfs takes more heap memory but dfs takes more stack memory and dfs is prone to stack overflow but dfs takes less overall memory because the space complexity is O(d) where d is the depth of the recursion while bfs takes O(w) where w where w is all the nodes in the tree in other word,dfs is deep while bfs is wide.

*so bfs is implemented with a while loop on a queue while dfs is implemented in a recursion under a for loop but my pre-order dfs is implemented as a recursion under a for loop but the for loop is outside of the for loop that searches the files meaning that bfs is implemented as an iteration over a queue and dfs as a recursion that uses the stack memory

iteration is separate from recursion

optional chaining,default value,null coalescing,non null value operator,type assertion

*so regardless of the sorting size,the bfs search will still finish at its time but the sorting order significantly impacts the speed at which results will be seen.sorting by size prioritizes responsiveness as the bfs will cover more breadth in less time but sorting by date oldest to newest can make the target result appear earlier but the bfs will cover less breadth in more time

*i believe that if i didnt put the check on the rootpath before performing the heuristic,nothing will be queued or processed till the root folder is deferred twice in a row.the root folder will be checked for relevance and if it doesnt,it will be deferred but there is no other element in the queue so the next iteration will access this rootpath again and defer it till it reaches the defer threshold before nything can be processed or if there want a deferred threshold,it will be deferred till infinity.so by adding this check,the rootpath and all its immediate children will always be queued,its the subfolders of its children that will have heuristic analysis on them.i was confused because i was wondering why my code didnt arrive at the root deferral i was thinking but the root check sorts it out.so in bfs,if i never give the queue a chance to grow,it will be in a deadlock or a stall at runtime.it will always be a single element till the root passes the conditions.so the check is necessary for the algorithm to work.

a priority queue is a queue but packed with heuristic analysis.instead of first com first serve,its most important,first served

*i believe that using a priority queue will get the job done but i think it will cause an extra O(n) of runtime to access the element in the queue with the highest relevance score.dynamically adjusting the relevance score seems easier to implement with lesser runtime.i can just push every deferred path to a an array that exists above the while loop called deferredPaths on every deferral so that the loop knows when its processing a deferred path,when it does,it knos it has comleted a cycle of paths,so it can reduce the relevance score to something lower on each cycle to 0.if i dont add this feature,then deferred paths will be processed in the order they were deferred


queue,priority queue,Binary heap
a heap is ideal for retaining values as sorted as they are inserted by doing gradual sorting

pre-order dfs--108 seconds no result
classic breadth first search--60 seconds till result
early best-first search--40 seconds till result

*so its unsafe to directly work on the internals of an abstracted data structure like the heap from heap-js without using their getters or setters

*the perf sky rocketed like never seen.the sort was the bottleneck.it searched in 10 seconds cuz of using the heap instead of sorting than using sorting that caused 40 seconds.but when i used classic bfs,i didnt see result till 60 seconds and when i used pre-order dfs,i didnt see results even after 100 seconds had passed.they all have 0(n) time compleity but the results of this early best-first is incredible.i taught that i had to use an indexer to make it this fast but this already covers that

*i taught that the extra runtime upfront for the heuristic calculation will negate perf but its morel like waiting a little,then instant results instead of longer waits.it wasnt 10 seconds.it was three.the reason for the extra 7 was because terminating the serach didnt disrupt the current dir level from being processed till the next level

*from 100 to 3 seconds without touching the impl.just changing the way the search approached the file system/search strategy

*although the indexer can get results instantly.my algorithm choice is still a top choice to avoid the pain of indexing.its just a few seconds different for the trade of easier maintainability and predictability.its always guaranteed that the search returns up to date data and that it will perform heuristics accurately at runtime

*i optimized it just now by removing the promise.all and awaiting each node as it loops.this ensures that the fsnode is only constructed when needed instead of all at once.it went from 3 seconds to 2 seconds.it felt almost instant:

*update search results internally batches updates.so the search can use the function to update the search result without knowing how the function does it under the hood

todo: eleven labs to perplexity

*,before,i had a separate foreach over the search results to push to the score inc but that added another O(n) worth of time to the function,so i merged that under the map that returns the result.item

*thats just a code opt.i just changed the way i wrote the code without changing the intention of the algorithm

*By checking if currentSearchPath === rootPath and then selecting fsNodes from your app state (instead of re-reading the directory), youâ€™ve eliminated redundant I/O and parsing for the most common case: searching the currently open directory. This is often the largest or most expensive directory, so the win is substantial.

*i leveraged the cache

from 100 seconds with no results to 60 seconds with results to 40 seconds to 10 seconds to 3 seconds and now to almost instant.

todo:caching the heuristic

todoLso for root folder,i should apply static heuristics where i defer known large folders like node_modules
todo:fix the arrow rendering in the home folder

the static heuristics worked but since it performs two cycles per level,the deferred node modules will be processed before going to the next level in the filesystem unless i push it to a queue outside of the searchinbreadth function and inside the caller which is searchDir.searchDir can then search that heavy folder queue after searching in general.this means unlike dynamic heuristics where deferred folders are processed before going one level deeper,static heuristics processes all deferred heavy folders after all search is complete.

*so perf shouldnt come at the cost of safety.Never sacrifice safety for a minor performance gain

so quick search wont consider heavy fplders in the next search cycle but full search will but they both defer it

*Deferred folders (low-relevance) are only processed after all non-deferred siblings at the same level have been processed.Children of deferred folders will not be processed until their parent is processed (i.e., after the main queue is empty and deferred paths are re-queued).If at least one folder at a level is not deferred, the queue will not empty, and deferred paths will wait.This creates a â€œlockâ€ effect:Deferred folders and their subtrees are â€œlockedâ€ out of processing until all immediate non-deferred siblings have been processed.

traversal order + search-time heuristics

*my search engine will get faster and more efficient over time as you refine and enhance your heuristics


quick search filters aggressively
quick search doesnt sort
quick search optimizes for long queries
quick search doesnt show progress
quick search doesnt run the second search loop

indexing,early best-first search,master file table search engines

*switching from membership testing to endswith  had to come at the cost of an O(n) but since the heavy folders are of a fixed size like just 6,its negligible and we can even say its O(1).not to mention this is more robust.i can mark arbritray paths as heavy not just specfici folder names

*so if a for loop iterates over an array of fixed size,its effectively O(1) especially if the array theoretical limit is small cuz the amount of heavy folders out there are negligible for the robustness

*so my static heuristics is O(1) but my dynamic one is O(n)

*so my javascript search engine is faster than the windows c++ search engine ot because of the underlying language although that can play a role but because of the algorithm i employed

*so i have two search loops the main loop and the heavy loop and the main loop is divided to deferred and relevant where relevant folders at a particular level are processed before deferred ones.the relevant children of a relevant folder are also processed before the deferred ones.

Levenshtein Distance
subsequence match
exact match
regex match
substring match

*so aggressive filter uses substring matching and isubsequence uses subsequence matching.the both normalize it to be a bit typo friendly but how they match are different.subsequence is more forgiving when it comes to missing chars.so what ill do is that quick search will continue to use the aggressive filter to filter out nodes on the fly during search processing and the heuristic will use subsequence for better relevance scores.substring reuires them to be contiguous but subsequence doesnt require them to be contiguous but in order.i have changed the heuristics now.for file names,it uses subseuqnce but for file extensions,it uses substring since file ext are better testing more precisely.this ensures that the subsequence is only O(n)+O(1) where n is the length of the query instead of O(n) + O(n)